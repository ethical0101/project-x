{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fefe61f",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning & Normalization\n",
    "\n",
    "This notebook implements the first step of our Frequent Pattern Mining project with Concept Lattice foundations.\n",
    "\n",
    "## Objectives\n",
    "- Load transaction data from JSON files\n",
    "- Clean and normalize transactions\n",
    "- Apply synonym normalization using a mapping file\n",
    "- Remove rare/meaningless items\n",
    "- Save cleaned transaction data for further processing\n",
    "\n",
    "## Theoretical Background\n",
    "\n",
    "**Data Cleaning** is a critical first step in any data mining process. For frequent pattern mining, we need to ensure:\n",
    "1. Consistent representation of items (case-sensitivity, whitespace, etc.)\n",
    "2. Removal of duplicates within transactions\n",
    "3. Standardization of synonyms and variants\n",
    "4. Identification and handling of rare items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff987a0",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "First, let's import all necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "857d9983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Set, Tuple, Any\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs('output', exist_ok=True)\n",
    "os.makedirs('figures', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238fdd9d",
   "metadata": {},
   "source": [
    "## Define Helper Functions\n",
    "\n",
    "Let's define functions to handle the data cleaning tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e60500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_file(file_path: str) -> Any:\n",
    "    \"\"\"Load data from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the JSON file\n",
    "\n",
    "    Returns:\n",
    "        Loaded data from the JSON file\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def save_json_file(data: Any, file_path: str) -> None:\n",
    "    \"\"\"Save data to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        data: Data to save\n",
    "        file_path: Path to the JSON file\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "\n",
    "def clean_and_normalize_transactions(\n",
    "    transactions: List[Dict[str, Any]],\n",
    "    normalization_map: Dict[str, str]\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Clean and normalize transaction data.\n",
    "\n",
    "    Args:\n",
    "        transactions: List of transaction dictionaries with 'transaction_id' and 'items' keys\n",
    "        normalization_map: Dictionary mapping raw item names to normalized names\n",
    "\n",
    "    Returns:\n",
    "        List of cleaned and normalized transactions\n",
    "    \"\"\"\n",
    "    cleaned_transactions = []\n",
    "    seen_ids = set()\n",
    "\n",
    "    for transaction in transactions:\n",
    "        transaction_id = transaction.get('transaction_id')\n",
    "\n",
    "        # Validate transaction_id\n",
    "        if transaction_id is None:\n",
    "            continue\n",
    "        if transaction_id in seen_ids:\n",
    "            print(f\"Warning: Duplicate transaction ID {transaction_id} found. Skipping.\")\n",
    "            continue\n",
    "        seen_ids.add(transaction_id)\n",
    "\n",
    "        # Clean and normalize items\n",
    "        items = transaction.get('items', [])\n",
    "        normalized_items = set()  # Use set to automatically deduplicate\n",
    "\n",
    "        for item in items:\n",
    "            if not item:  # Skip empty items\n",
    "                continue\n",
    "\n",
    "            # Lowercase and strip\n",
    "            clean_item = item.lower().strip()\n",
    "\n",
    "            # Apply normalization map\n",
    "            normalized_item = normalization_map.get(item, normalization_map.get(clean_item, clean_item))\n",
    "\n",
    "            if normalized_item:  # Add non-empty items\n",
    "                normalized_items.add(normalized_item)\n",
    "\n",
    "        if normalized_items:  # Only add transactions with at least one item\n",
    "            cleaned_transactions.append({\n",
    "                'transaction_id': transaction_id,\n",
    "                'items': sorted(list(normalized_items))  # Convert back to sorted list\n",
    "            })\n",
    "\n",
    "    return cleaned_transactions\n",
    "\n",
    "\n",
    "def filter_rare_items(\n",
    "    transactions: List[Dict[str, Any]],\n",
    "    threshold: float = 0.05\n",
    ") -> Tuple[List[Dict[str, Any]], Set[str]]:\n",
    "    \"\"\"Remove items that appear less frequently than the threshold.\n",
    "\n",
    "    Args:\n",
    "        transactions: List of transaction dictionaries\n",
    "        threshold: Minimum frequency threshold (default: 0.05 or 5%)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (filtered transactions, set of removed items)\n",
    "    \"\"\"\n",
    "    # Count item frequencies\n",
    "    item_counts = {}\n",
    "    for transaction in transactions:\n",
    "        for item in transaction['items']:\n",
    "            item_counts[item] = item_counts.get(item, 0) + 1\n",
    "\n",
    "    # Identify rare items\n",
    "    total_transactions = len(transactions)\n",
    "    rare_items = {\n",
    "        item for item, count in item_counts.items()\n",
    "        if count / total_transactions < threshold\n",
    "    }\n",
    "\n",
    "    # Remove rare items from transactions\n",
    "    filtered_transactions = []\n",
    "    for transaction in transactions:\n",
    "        filtered_items = [item for item in transaction['items'] if item not in rare_items]\n",
    "        if filtered_items:  # Only include transactions with at least one item left\n",
    "            filtered_transactions.append({\n",
    "                'transaction_id': transaction['transaction_id'],\n",
    "                'items': filtered_items\n",
    "            })\n",
    "\n",
    "    return filtered_transactions, rare_items\n",
    "\n",
    "\n",
    "def transactions_to_lists(transactions: List[Dict[str, Any]]) -> List[List[str]]:\n",
    "    \"\"\"Convert transaction dictionaries to lists of items.\n",
    "\n",
    "    Args:\n",
    "        transactions: List of transaction dictionaries\n",
    "\n",
    "    Returns:\n",
    "        List of item lists\n",
    "    \"\"\"\n",
    "    return [transaction['items'] for transaction in transactions]\n",
    "\n",
    "\n",
    "def get_unique_items(transactions: List[Dict[str, Any]]) -> Set[str]:\n",
    "    \"\"\"Get set of all unique items across transactions.\n",
    "\n",
    "    Args:\n",
    "        transactions: List of transaction dictionaries\n",
    "\n",
    "    Returns:\n",
    "        Set of unique items\n",
    "    \"\"\"\n",
    "    unique_items = set()\n",
    "    for transaction in transactions:\n",
    "        unique_items.update(transaction['items'])\n",
    "    return unique_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2869a63",
   "metadata": {},
   "source": [
    "## Load Raw Data\n",
    "\n",
    "Now, let's load the raw transaction data and the normalization mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d0b5d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for file at: e:\\project-x\\data\\transactions.json\n",
      "Loaded 20 transactions from your real data!\n",
      "\n",
      "Your Real Transaction Data (showing first 5 of 20):\n",
      "1: ['Apple', 'milk', 'bread']\n",
      "2: ['rice', 'Oil', 'beans']\n",
      "3: ['milk', 'eggs', 'cheese', 'yogurt']\n",
      "4: ['bread', 'butter', 'milk']\n",
      "5: ['apple', 'banana', 'orange', 'grapes']\n"
     ]
    }
   ],
   "source": [
    "# Load transactions from a JSON file\n",
    "import os\n",
    "\n",
    "# Get absolute paths - fix path for notebook location\n",
    "current_dir = os.getcwd()\n",
    "# Check if we're in notebooks folder, if so go up one level\n",
    "if current_dir.endswith('notebooks'):\n",
    "    project_dir = os.path.dirname(current_dir)\n",
    "else:\n",
    "    project_dir = current_dir\n",
    "\n",
    "transaction_file = os.path.join(project_dir, 'data', 'transactions.json')\n",
    "print(f\"Looking for file at: {transaction_file}\")\n",
    "\n",
    "try:\n",
    "    with open(transaction_file, 'r', encoding='utf-8') as f:\n",
    "        transactions = json.load(f)\n",
    "    print(f\"Loaded {len(transactions)} transactions from your real data!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Transaction file not found at {transaction_file}.\")\n",
    "    print(\"Creating sample transactions...\")\n",
    "\n",
    "    # Create sample transactions\n",
    "    sample_transactions = [\n",
    "        {\"transaction_id\": 1, \"items\": [\"Apple\", \"Milk\", \"Bread\"]},\n",
    "        {\"transaction_id\": 2, \"items\": [\"Rice\", \"Oil\"]},\n",
    "        {\"transaction_id\": 3, \"items\": [\"milk\", \"Eggs\", \"Cheese\"]},\n",
    "        {\"transaction_id\": 4, \"items\": [\"Bread\", \"Butter\", \"Milk\"]},\n",
    "        {\"transaction_id\": 5, \"items\": [\"apple\", \"Banana\", \"Orange\"]}\n",
    "    ]\n",
    "\n",
    "    # Save sample transactions\n",
    "    os.makedirs(os.path.dirname(transaction_file), exist_ok=True)\n",
    "    with open(transaction_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sample_transactions, f, indent=2)\n",
    "\n",
    "    transactions = sample_transactions\n",
    "    print(f\"Created and saved {len(transactions)} sample transactions\")\n",
    "\n",
    "# Display a few transactions\n",
    "print(f\"\\nYour Real Transaction Data (showing first 5 of {len(transactions)}):\")\n",
    "for i, transaction in enumerate(transactions[:5]):\n",
    "    print(f\"{transaction['transaction_id']}: {transaction['items']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f72793",
   "metadata": {},
   "source": [
    "## Clean and Normalize Transactions\n",
    "\n",
    "Apply the cleaning and normalization process to the raw transactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "95ac8860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for normalization file at: e:\\project-x\\data\\normalization.json\n",
      "Loaded normalization map with 33 entries\n"
     ]
    }
   ],
   "source": [
    "# Load normalization map\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Get the correct path for normalization file\n",
    "current_dir = os.getcwd()\n",
    "# Check if we're in notebooks folder, if so go up one level\n",
    "if current_dir.endswith('notebooks'):\n",
    "    project_dir = os.path.dirname(current_dir)\n",
    "else:\n",
    "    project_dir = current_dir\n",
    "\n",
    "normalization_path = os.path.join(project_dir, 'data', 'normalization.json')\n",
    "\n",
    "try:\n",
    "    print(f\"Looking for normalization file at: {normalization_path}\")\n",
    "    with open(normalization_path, 'r') as f:\n",
    "        normalization_map = json.load(f)\n",
    "    print(f\"Loaded normalization map with {len(normalization_map)} entries\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Normalization file not found at {normalization_path}\")\n",
    "    # Create a basic normalization map if file not found\n",
    "    normalization_map = {\n",
    "        \"apple\": \"apple\",\n",
    "        \"Apple\": \"apple\",\n",
    "        \"milk\": \"milk\",\n",
    "        \"Milk\": \"milk\",\n",
    "        \"bread\": \"bread\",\n",
    "        \"Bread\": \"bread\",\n",
    "        \"rice\": \"rice\",\n",
    "        \"Rice\": \"rice\",\n",
    "        \"oil\": \"oil\",\n",
    "        \"Oil\": \"oil\"\n",
    "    }\n",
    "    print(f\"Created default normalization map with {len(normalization_map)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a7f30b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to clean and normalize transactions\n",
    "def clean_and_normalize_transactions(transactions, normalization_map):\n",
    "    \"\"\"\n",
    "    Clean and normalize transaction data.\n",
    "\n",
    "    Args:\n",
    "        transactions: List of transaction dictionaries with 'items' field\n",
    "        normalization_map: Dictionary mapping item variants to normalized names\n",
    "\n",
    "    Returns:\n",
    "        List of cleaned transactions\n",
    "    \"\"\"\n",
    "    cleaned_transactions = []\n",
    "\n",
    "    for transaction in transactions:\n",
    "        cleaned_items = []\n",
    "\n",
    "        for item in transaction[\"items\"]:\n",
    "            # Apply normalization if available\n",
    "            normalized_item = normalization_map.get(item, item.lower())\n",
    "            cleaned_items.append(normalized_item)\n",
    "\n",
    "        # Remove duplicates (if an item appears multiple times in a transaction)\n",
    "        cleaned_items = list(set(cleaned_items))\n",
    "\n",
    "        # Add to cleaned transactions\n",
    "        cleaned_transactions.append({\n",
    "            \"transaction_id\": transaction.get(\"transaction_id\", len(cleaned_transactions) + 1),\n",
    "            \"items\": cleaned_items\n",
    "        })\n",
    "\n",
    "    return cleaned_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac72f99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: 20 transactions\n",
      "\n",
      "Cleaned Transaction Sample:\n",
      "{'transaction_id': 1, 'items': ['milk', 'bread', 'apple']}\n",
      "{'transaction_id': 2, 'items': ['beans', 'oil', 'rice']}\n",
      "{'transaction_id': 3, 'items': ['yogurt', 'milk', 'cheese', 'eggs']}\n",
      "\n",
      "Number of unique items after cleaning: 25\n",
      "Unique items: ['apple', 'bacon', 'banana', 'beans', 'berries', 'bread', 'butter', 'cereal', 'cheese', 'chicken', 'coffee', 'crackers', 'eggs', 'grapes', 'jam', 'milk', 'oil', 'orange', 'rice', 'sugar', 'tea', 'tomato', 'vegetables', 'wine', 'yogurt']\n"
     ]
    }
   ],
   "source": [
    "# Clean and normalize transactions\n",
    "cleaned_transactions = clean_and_normalize_transactions(transactions, normalization_map)\n",
    "\n",
    "print(f\"After cleaning: {len(cleaned_transactions)} transactions\")\n",
    "\n",
    "# Show a sample of the cleaned data\n",
    "print(\"\\nCleaned Transaction Sample:\")\n",
    "for t in cleaned_transactions[:3]:\n",
    "    print(t)\n",
    "\n",
    "# Count unique items after cleaning\n",
    "unique_items = get_unique_items(cleaned_transactions)\n",
    "print(f\"\\nNumber of unique items after cleaning: {len(unique_items)}\")\n",
    "print(\"Unique items:\", sorted(list(unique_items)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3607e86",
   "metadata": {},
   "source": [
    "## Analyze Item Frequencies\n",
    "\n",
    "Let's analyze the frequency of items in the transactions to identify rare items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "caa16513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item Frequencies:\n",
      "          item  count  frequency\n",
      "0         milk     10       0.50\n",
      "1        bread      6       0.30\n",
      "2       banana      5       0.25\n",
      "3       cheese      4       0.20\n",
      "4         rice      4       0.20\n",
      "5       yogurt      3       0.15\n",
      "6         eggs      3       0.15\n",
      "7       butter      3       0.15\n",
      "8        apple      3       0.15\n",
      "9        beans      2       0.10\n",
      "10         oil      2       0.10\n",
      "11      orange      2       0.10\n",
      "12  vegetables      2       0.10\n",
      "13      coffee      2       0.10\n",
      "14     chicken      2       0.10\n",
      "15      cereal      2       0.10\n",
      "16       bacon      2       0.10\n",
      "17         tea      1       0.05\n",
      "18      grapes      1       0.05\n",
      "19      tomato      1       0.05\n",
      "20     berries      1       0.05\n",
      "21       sugar      1       0.05\n",
      "22        wine      1       0.05\n",
      "23    crackers      1       0.05\n",
      "24         jam      1       0.05\n",
      "\n",
      "Item frequencies calculated. Skipping visualization due to matplotlib compatibility issue.\n",
      "\n",
      "Rare items (frequency < 0.2):\n",
      "['yogurt', 'eggs', 'butter', 'apple', 'beans', 'oil', 'orange', 'vegetables', 'coffee', 'chicken', 'cereal', 'bacon', 'tea', 'grapes', 'tomato', 'berries', 'sugar', 'wine', 'crackers', 'jam']\n",
      "\n",
      "Common items (frequency >= 0.2):\n",
      "['milk', 'bread', 'banana', 'cheese', 'rice']\n"
     ]
    }
   ],
   "source": [
    "# Calculate item frequencies\n",
    "item_counts = {}\n",
    "for transaction in cleaned_transactions:\n",
    "    for item in transaction[\"items\"]:\n",
    "        if item not in item_counts:\n",
    "            item_counts[item] = 0\n",
    "        item_counts[item] += 1\n",
    "\n",
    "# Create DataFrame for easier analysis\n",
    "import pandas as pd\n",
    "items = list(item_counts.keys())\n",
    "counts = list(item_counts.values())\n",
    "frequencies = [count / len(cleaned_transactions) for count in counts]\n",
    "\n",
    "item_freq_df = pd.DataFrame({\n",
    "    'item': items,\n",
    "    'count': counts,\n",
    "    'frequency': frequencies\n",
    "})\n",
    "\n",
    "# Sort by frequency\n",
    "item_freq_df = item_freq_df.sort_values('count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Item Frequencies:\")\n",
    "print(item_freq_df)\n",
    "\n",
    "# Skip visualization due to matplotlib compatibility issue\n",
    "print(\"\\nItem frequencies calculated. Skipping visualization due to matplotlib compatibility issue.\")\n",
    "\n",
    "# Filter rare items (let's say items that appear in less than 20% of transactions)\n",
    "rare_item_threshold = 0.2\n",
    "rare_items = item_freq_df[item_freq_df['frequency'] < rare_item_threshold]['item'].tolist()\n",
    "common_items = item_freq_df[item_freq_df['frequency'] >= rare_item_threshold]['item'].tolist()\n",
    "\n",
    "print(f\"\\nRare items (frequency < {rare_item_threshold}):\")\n",
    "print(rare_items)\n",
    "print(f\"\\nCommon items (frequency >= {rare_item_threshold}):\")\n",
    "print(common_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b33609",
   "metadata": {},
   "source": [
    "## Filter Rare Items\n",
    "\n",
    "Now, let's filter out items that appear in less than 5% of the transactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3666e554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file not found or invalid. Using default rare_item_threshold of 0.05.\n",
      "Using rare item threshold: 0.05 (5.0%)\n",
      "After filtering rare items: 20 transactions\n",
      "Removed 0 rare items: []\n",
      "Number of unique items after filtering: 25\n",
      "Remaining items: ['apple', 'bacon', 'banana', 'beans', 'berries', 'bread', 'butter', 'cereal', 'cheese', 'chicken', 'coffee', 'crackers', 'eggs', 'grapes', 'jam', 'milk', 'oil', 'orange', 'rice', 'sugar', 'tea', 'tomato', 'vegetables', 'wine', 'yogurt']\n"
     ]
    }
   ],
   "source": [
    "# Load configuration (if available)\n",
    "config_path = 'config.json'\n",
    "\n",
    "try:\n",
    "    config = load_json_file(config_path)\n",
    "    rare_item_threshold = config.get('rare_item_threshold', 0.05)\n",
    "except (FileNotFoundError, json.JSONDecodeError):\n",
    "    print(\"Config file not found or invalid. Using default rare_item_threshold of 0.05.\")\n",
    "    rare_item_threshold = 0.05\n",
    "\n",
    "print(f\"Using rare item threshold: {rare_item_threshold} ({rare_item_threshold*100}%)\")\n",
    "\n",
    "# Filter rare items\n",
    "filtered_transactions, rare_items = filter_rare_items(\n",
    "    cleaned_transactions,\n",
    "    threshold=rare_item_threshold\n",
    ")\n",
    "\n",
    "print(f\"After filtering rare items: {len(filtered_transactions)} transactions\")\n",
    "print(f\"Removed {len(rare_items)} rare items: {sorted(list(rare_items))}\")\n",
    "\n",
    "# Count unique items after filtering\n",
    "unique_items_after_filtering = get_unique_items(filtered_transactions)\n",
    "print(f\"Number of unique items after filtering: {len(unique_items_after_filtering)}\")\n",
    "print(\"Remaining items:\", sorted(list(unique_items_after_filtering)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2379a19c",
   "metadata": {},
   "source": [
    "## Save Cleaned Data\n",
    "\n",
    "Save the cleaned data for use in subsequent steps of the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b25a22ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned transactions to e:\\project-x\\output\\cleaned_transactions.json\n",
      "Saved rare items list to e:\\project-x\\output\\rare_items.json\n",
      "Saved transaction lists to e:\\project-x\\output\\transaction_lists.json\n",
      "\n",
      "Final Transaction Lists Sample (from your 20 real transactions):\n",
      "['milk', 'bread', 'apple']\n",
      "['beans', 'oil', 'rice']\n",
      "['yogurt', 'milk', 'cheese', 'eggs']\n",
      "['bread', 'milk', 'butter']\n",
      "['grapes', 'orange', 'apple', 'banana']\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned transactions as JSON\n",
    "# Get the correct output path\n",
    "current_dir = os.getcwd()\n",
    "if current_dir.endswith('notebooks'):\n",
    "    project_dir = os.path.dirname(current_dir)\n",
    "else:\n",
    "    project_dir = current_dir\n",
    "\n",
    "output_path = os.path.join(project_dir, 'output', 'cleaned_transactions.json')\n",
    "save_json_file(filtered_transactions, output_path)\n",
    "print(f\"Saved cleaned transactions to {output_path}\")\n",
    "\n",
    "# Save rare items list\n",
    "rare_items_path = os.path.join(project_dir, 'output', 'rare_items.json')\n",
    "save_json_file(list(rare_items), rare_items_path)\n",
    "print(f\"Saved rare items list to {rare_items_path}\")\n",
    "\n",
    "# Save as list format for next steps\n",
    "transaction_lists = transactions_to_lists(filtered_transactions)\n",
    "lists_path = os.path.join(project_dir, 'output', 'transaction_lists.json')\n",
    "save_json_file(transaction_lists, lists_path)\n",
    "print(f\"Saved transaction lists to {lists_path}\")\n",
    "\n",
    "# Show a sample of the final output\n",
    "print(f\"\\nFinal Transaction Lists Sample (from your {len(transaction_lists)} real transactions):\")\n",
    "for t in transaction_lists[:5]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59269345",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "\n",
    "Let's calculate some summary statistics for our cleaned dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8c28d81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transactions: 20\n",
      "Number of unique items: 25\n",
      "Average items per transaction: 3.25\n",
      "Median items per transaction: 3.00\n",
      "Maximum items in a transaction: 4\n",
      "Minimum items in a transaction: 3\n",
      "\n",
      "Skipping visualization due to matplotlib compatibility issue.\n",
      "\n",
      "Saved cleaned transactions to CSV file: ../output\\cleaned_transactions.csv\n",
      "\n",
      "Transactions DataFrame (head):\n",
      "   transaction_id    item\n",
      "0               1    milk\n",
      "1               1   bread\n",
      "2               1   apple\n",
      "3               2   beans\n",
      "4               2     oil\n",
      "5               2    rice\n",
      "6               3  yogurt\n",
      "7               3    milk\n",
      "8               3  cheese\n",
      "9               3    eggs\n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrame and save as CSV\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Calculate statistics\n",
    "transaction_lengths = [len(transaction) for transaction in transaction_lists]\n",
    "avg_items_per_transaction = sum(transaction_lengths) / len(transaction_lengths)\n",
    "median_items_per_transaction = sorted(transaction_lengths)[len(transaction_lengths) // 2]\n",
    "max_items_per_transaction = max(transaction_lengths)\n",
    "min_items_per_transaction = min(transaction_lengths)\n",
    "\n",
    "print(f\"Number of transactions: {len(transaction_lists)}\")\n",
    "print(f\"Number of unique items: {len(set(item for sublist in transaction_lists for item in sublist))}\")\n",
    "print(f\"Average items per transaction: {avg_items_per_transaction:.2f}\")\n",
    "print(f\"Median items per transaction: {median_items_per_transaction:.2f}\")\n",
    "print(f\"Maximum items in a transaction: {max_items_per_transaction}\")\n",
    "print(f\"Minimum items in a transaction: {min_items_per_transaction}\")\n",
    "\n",
    "# Skip visualization due to matplotlib compatibility issue\n",
    "print(\"\\nSkipping visualization due to matplotlib compatibility issue.\")\n",
    "\n",
    "# Create a list of dictionaries for each transaction\n",
    "transaction_dicts = []\n",
    "for i, items in enumerate(transaction_lists):\n",
    "    for item in items:\n",
    "        transaction_dicts.append({\n",
    "            'transaction_id': i + 1,\n",
    "            'item': item\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "transactions_df = pd.DataFrame(transaction_dicts)\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = os.path.join('../output', 'cleaned_transactions.csv')\n",
    "transactions_df.to_csv(csv_path, index=False)\n",
    "print(f\"\\nSaved cleaned transactions to CSV file: {csv_path}\")\n",
    "\n",
    "# Display the head of the DataFrame\n",
    "print(\"\\nTransactions DataFrame (head):\")\n",
    "print(transactions_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf6df96",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we successfully:\n",
    "\n",
    "1. Loaded raw transaction data from JSON files\n",
    "2. Cleaned and normalized the data (lowercasing, stripping, deduplication)\n",
    "3. Applied synonym normalization using a mapping file\n",
    "4. Analyzed item frequencies and filtered rare items\n",
    "5. Saved the cleaned data for further processing\n",
    "\n",
    "The cleaned dataset is now ready for the next steps in the frequent pattern mining pipeline:\n",
    "- Transaction encoding\n",
    "- Frequent itemset mining with Apriori and FP-Growth\n",
    "- Closed itemset computation and concept lattice construction\n",
    "\n",
    "The data cleaning process is crucial as it directly impacts the quality and interpretability of the patterns we will discover later. By standardizing item representations and removing rare items, we improve the meaningfulness of the frequent patterns while reducing computational complexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
