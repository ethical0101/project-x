{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7acad24f",
   "metadata": {},
   "source": [
    "# 3. Concept Lattice and Closed Itemsets\n",
    "\n",
    "This notebook implements the concept lattice construction and closed itemset mining parts of our Frequent Pattern Mining project.\n",
    "\n",
    "## Objectives\n",
    "- Load encoded transaction data\n",
    "- Compute closed itemsets using a closure operator\n",
    "- Construct a concept lattice (Hasse diagram)\n",
    "- Visualize the concept lattice\n",
    "- Generate concise rules from closed itemsets\n",
    "- Compare with rules from traditional algorithms\n",
    "\n",
    "## Theoretical Background\n",
    "\n",
    "**Formal Concept Analysis (FCA)** is a mathematical theory that provides a framework for discovering and representing conceptual knowledge. In the context of frequent pattern mining:\n",
    "\n",
    "1. **Closed Itemset**: An itemset that is equal to its closure. The closure of an itemset $I$ is the set of all items that occur in every transaction containing $I$.\n",
    "\n",
    "2. **Concept Lattice**: A hierarchical structure where:\n",
    "   - Each node represents a formal concept (closed itemset with its supporting transactions)\n",
    "   - Edges represent subset/superset relationships between concepts\n",
    "   - The lattice captures all possible closed itemsets and their relationships\n",
    "\n",
    "3. **Galois Connection**: The pair of mappings between:\n",
    "   - Items → Transactions (containing all those items)\n",
    "   - Transactions → Items (common to all those transactions)\n",
    "\n",
    "4. **Minimal Generators**: The smallest itemsets that have the same closure as a given closed itemset. These are useful for generating concise rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b187c9e2",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4763433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\n",
    "from typing import Dict, List, Set, Tuple, Any, FrozenSet\n",
    "from itertools import combinations\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs('output', exist_ok=True)\n",
    "os.makedirs('figures', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be1c157",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893ac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_file(file_path: str) -> Any:\n",
    "    \"\"\"Load data from a JSON file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def save_json_file(data: Any, file_path: str) -> None:\n",
    "    \"\"\"Save data to a JSON file.\"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "\n",
    "def frozenset_to_list(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert frozenset columns in a DataFrame to lists for easier handling.\"\"\"\n",
    "    result = df.copy()\n",
    "\n",
    "    # Convert any frozenset columns to lists\n",
    "    for col in result.columns:\n",
    "        if result[col].apply(lambda x: isinstance(x, frozenset)).any():\n",
    "            result[col] = result[col].apply(lambda x: list(x) if isinstance(x, frozenset) else x)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92dc443",
   "metadata": {},
   "source": [
    "## Load Transaction Data\n",
    "\n",
    "First, let's load the encoded transaction data from the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22458bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load encoded transaction data\n",
    "encoded_path = 'output/encoded_transactions.csv'\n",
    "\n",
    "try:\n",
    "    encoded_df = pd.read_csv(encoded_path)\n",
    "    print(f\"Loaded encoded transactions: {encoded_df.shape}\")\n",
    "\n",
    "    # Display a sample of the encoded data\n",
    "    print(\"\\nSample of Encoded Transactions:\")\n",
    "    print(encoded_df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Encoded transactions file not found at {encoded_path}\")\n",
    "    print(\"Creating a sample encoded dataset...\")\n",
    "\n",
    "    # Load transaction lists\n",
    "    try:\n",
    "        transaction_lists = load_json_file('output/transaction_lists.json')\n",
    "    except FileNotFoundError:\n",
    "        # Create sample transactions if needed\n",
    "        transaction_lists = [\n",
    "            [\"apple\", \"milk\", \"bread\"],\n",
    "            [\"rice\", \"oil\"],\n",
    "            [\"milk\", \"eggs\", \"cheese\"],\n",
    "            [\"bread\", \"butter\", \"milk\"],\n",
    "            [\"apple\", \"banana\", \"orange\"]\n",
    "        ]\n",
    "\n",
    "    # Encode the transactions\n",
    "    from mlxtend.preprocessing import TransactionEncoder\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit_transform(transaction_lists)\n",
    "    encoded_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "    print(f\"Created sample encoded transactions: {encoded_df.shape}\")\n",
    "\n",
    "# Load configuration\n",
    "try:\n",
    "    config = load_json_file('config.json')\n",
    "    min_support = config.get('min_support', 0.2)\n",
    "    min_confidence = config.get('min_confidence', 0.6)\n",
    "except (FileNotFoundError, json.JSONDecodeError):\n",
    "    print(\"Config file not found or invalid. Using default parameters.\")\n",
    "    min_support = 0.2\n",
    "    min_confidence = 0.6\n",
    "\n",
    "print(f\"\\nParameters:\")\n",
    "print(f\"  Min Support: {min_support}\")\n",
    "print(f\"  Min Confidence: {min_confidence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dadd0f3",
   "metadata": {},
   "source": [
    "## Concept Lattice Implementation\n",
    "\n",
    "Now, let's implement the ConceptLattice class that will compute closed itemsets and build the lattice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73df41a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptLattice:\n",
    "    \"\"\"Implementation of a concept lattice based on closed itemsets.\"\"\"\n",
    "\n",
    "    def __init__(self, transaction_df: pd.DataFrame):\n",
    "        \"\"\"Initialize the concept lattice with encoded transaction data.\"\"\"\n",
    "        self.transaction_df = transaction_df\n",
    "        self.item_names = list(transaction_df.columns)\n",
    "        self.intents = {}  # Maps extent (transaction indices) to intent (items)\n",
    "        self.extents = {}  # Maps intent (items) to extent (transaction indices)\n",
    "        self.closed_itemsets = []\n",
    "        self.lattice = None\n",
    "\n",
    "    def compute_closed_itemsets(self) -> List[Tuple[FrozenSet, float]]:\n",
    "        \"\"\"Compute closed itemsets using the closure operator.\n",
    "\n",
    "        Returns:\n",
    "            List of (closed_itemset, support) tuples\n",
    "        \"\"\"\n",
    "        # Get all transactions as sets of items\n",
    "        transactions = []\n",
    "        for _, row in self.transaction_df.iterrows():\n",
    "            transaction = frozenset([\n",
    "                self.item_names[i] for i, value in enumerate(row) if value\n",
    "            ])\n",
    "            transactions.append(transaction)\n",
    "\n",
    "        # Build the context\n",
    "        objects = list(range(len(transactions)))\n",
    "        attributes = set()\n",
    "        for trans in transactions:\n",
    "            attributes.update(trans)\n",
    "\n",
    "        # Initialize dictionaries for extent->intent and intent->extent mappings\n",
    "        self.extents = {}\n",
    "        self.intents = {}\n",
    "\n",
    "        # Compute closure for each transaction (object)\n",
    "        for i, transaction in enumerate(transactions):\n",
    "            # Find all objects that share these attributes\n",
    "            extent = frozenset([\n",
    "                obj for obj, trans in enumerate(transactions)\n",
    "                if transaction.issubset(trans)\n",
    "            ])\n",
    "\n",
    "            # Find the common attributes for these objects\n",
    "            intent = frozenset(attributes)\n",
    "            for obj in extent:\n",
    "                intent = intent.intersection(transactions[obj])\n",
    "\n",
    "            self.extents[intent] = extent\n",
    "            self.intents[extent] = intent\n",
    "\n",
    "        # Compute closures for all possible subsets of attributes (naively)\n",
    "        # Note: For large datasets, more efficient algorithms like Charm or LCM should be used\n",
    "        closed_itemsets_with_support = []\n",
    "\n",
    "        # Start with the empty set\n",
    "        empty_intent = frozenset()\n",
    "        empty_extent = frozenset([\n",
    "            i for i, _ in enumerate(transactions)\n",
    "        ])\n",
    "        self.extents[empty_intent] = empty_extent\n",
    "        self.intents[empty_extent] = empty_intent\n",
    "\n",
    "        # For each transaction, compute its closure\n",
    "        for trans in transactions:\n",
    "            # Compute extent - objects containing all items in trans\n",
    "            extent = frozenset([\n",
    "                i for i, t in enumerate(transactions)\n",
    "                if trans.issubset(t)\n",
    "            ])\n",
    "\n",
    "            # Compute intent - items common to all objects in extent\n",
    "            intent = frozenset(attributes)\n",
    "            for obj in extent:\n",
    "                intent = intent.intersection(transactions[obj])\n",
    "\n",
    "            # Add to dictionaries if not already present\n",
    "            self.extents[intent] = extent\n",
    "            self.intents[extent] = intent\n",
    "\n",
    "        # Get unique closed itemsets from the extents dictionary\n",
    "        for intent, extent in self.extents.items():\n",
    "            support = len(extent) / len(transactions)\n",
    "            if support >= min_support:\n",
    "                closed_itemsets_with_support.append((intent, support))\n",
    "\n",
    "        self.closed_itemsets = sorted(\n",
    "            closed_itemsets_with_support,\n",
    "            key=lambda x: (len(x[0]), x[1]),\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        return self.closed_itemsets\n",
    "\n",
    "    def build_lattice(self) -> nx.DiGraph:\n",
    "        \"\"\"Build the concept lattice (Hasse diagram) from closed itemsets.\n",
    "\n",
    "        Returns:\n",
    "            NetworkX DiGraph representing the lattice\n",
    "        \"\"\"\n",
    "        if not self.closed_itemsets:\n",
    "            self.compute_closed_itemsets()\n",
    "\n",
    "        # Sort closed itemsets by size for easier processing\n",
    "        sorted_itemsets = sorted(\n",
    "            self.closed_itemsets,\n",
    "            key=lambda x: len(x[0])\n",
    "        )\n",
    "\n",
    "        # Create graph\n",
    "        G = nx.DiGraph()\n",
    "\n",
    "        # Add nodes for each closed itemset\n",
    "        for i, (itemset, support) in enumerate(sorted_itemsets):\n",
    "            label = f\"{{{', '.join(sorted(itemset))}}}\\nSupport: {support:.2f}\"\n",
    "            G.add_node(i, itemset=itemset, support=support, label=label)\n",
    "\n",
    "        # Add edges from each itemset to its minimal supersets\n",
    "        for i, (itemset1, _) in enumerate(sorted_itemsets):\n",
    "            for j, (itemset2, _) in enumerate(sorted_itemsets):\n",
    "                # Check if itemset2 is a proper superset of itemset1\n",
    "                if itemset1.issubset(itemset2) and itemset1 != itemset2:\n",
    "                    # Check if there's no intermediate itemset between itemset1 and itemset2\n",
    "                    is_minimal = True\n",
    "                    for k, (itemset3, _) in enumerate(sorted_itemsets):\n",
    "                        if (itemset1.issubset(itemset3) and\n",
    "                            itemset3.issubset(itemset2) and\n",
    "                            itemset1 != itemset3 and\n",
    "                            itemset3 != itemset2):\n",
    "                            is_minimal = False\n",
    "                            break\n",
    "\n",
    "                    if is_minimal:\n",
    "                        G.add_edge(i, j)\n",
    "\n",
    "        self.lattice = G\n",
    "        return G\n",
    "\n",
    "    def visualize_lattice(\n",
    "        self,\n",
    "        output_path: str = 'figures/lattice.png',\n",
    "        node_size: int = 1000,\n",
    "        font_size: int = 8,\n",
    "        edge_width: float = 1.5\n",
    "    ) -> None:\n",
    "        \"\"\"Visualize the concept lattice.\"\"\"\n",
    "        if self.lattice is None:\n",
    "            self.build_lattice()\n",
    "\n",
    "        # Set up the figure\n",
    "        plt.figure(figsize=(12, 10))\n",
    "\n",
    "        # Use hierarchical layout\n",
    "        pos = nx.spring_layout(self.lattice, k=2, iterations=500, seed=42)\n",
    "\n",
    "        # Draw the graph\n",
    "        nx.draw_networkx_edges(\n",
    "            self.lattice,\n",
    "            pos,\n",
    "            width=edge_width,\n",
    "            arrowstyle='->',\n",
    "            arrowsize=15\n",
    "        )\n",
    "\n",
    "        # Color nodes by support\n",
    "        supports = [data['support'] for _, data in self.lattice.nodes(data=True)]\n",
    "\n",
    "        # Normalize supports for coloring\n",
    "        max_support = max(supports) if supports else 1.0\n",
    "        min_support = min(supports) if supports else 0.0\n",
    "        range_support = max_support - min_support if max_support > min_support else 1.0\n",
    "\n",
    "        norm_supports = [\n",
    "            (s - min_support) / range_support\n",
    "            for s in supports\n",
    "        ]\n",
    "\n",
    "        # Draw nodes with support-based colors\n",
    "        nx.draw_networkx_nodes(\n",
    "            self.lattice,\n",
    "            pos,\n",
    "            node_size=node_size,\n",
    "            node_color=norm_supports,\n",
    "            cmap=plt.cm.Blues,\n",
    "            alpha=0.8\n",
    "        )\n",
    "\n",
    "        # Draw labels\n",
    "        labels = {\n",
    "            node: data['label'] for node, data in self.lattice.nodes(data=True)\n",
    "        }\n",
    "\n",
    "        nx.draw_networkx_labels(\n",
    "            self.lattice,\n",
    "            pos,\n",
    "            labels=labels,\n",
    "            font_size=font_size,\n",
    "            font_family='sans-serif',\n",
    "            font_weight='bold'\n",
    "        )\n",
    "\n",
    "        # Save the figure\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def generate_concise_rules(self, min_confidence: float = 0.6) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Generate concise rules from closed itemsets.\"\"\"\n",
    "        if not self.closed_itemsets:\n",
    "            self.compute_closed_itemsets()\n",
    "\n",
    "        rules = []\n",
    "\n",
    "        # Process each closed itemset\n",
    "        for itemset, support in self.closed_itemsets:\n",
    "            if len(itemset) <= 1:  # Skip singletons\n",
    "                continue\n",
    "\n",
    "            # Generate all proper subsets as antecedents\n",
    "            for i in range(1, len(itemset)):\n",
    "                # Generate all i-sized subsets\n",
    "                for subset in self._generate_subsets(itemset, i):\n",
    "                    antecedent = frozenset(subset)\n",
    "                    consequent = frozenset(itemset - antecedent)\n",
    "\n",
    "                    # Find support of the antecedent\n",
    "                    antecedent_support = self._get_itemset_support(antecedent)\n",
    "\n",
    "                    if antecedent_support > 0:\n",
    "                        # Calculate confidence\n",
    "                        confidence = support / antecedent_support\n",
    "\n",
    "                        # Calculate lift\n",
    "                        consequent_support = self._get_itemset_support(consequent)\n",
    "                        lift = confidence / consequent_support if consequent_support > 0 else 0\n",
    "\n",
    "                        # Add rule if it meets the confidence threshold\n",
    "                        if confidence >= min_confidence:\n",
    "                            rules.append({\n",
    "                                'antecedent': list(antecedent),\n",
    "                                'consequent': list(consequent),\n",
    "                                'support': support,\n",
    "                                'confidence': confidence,\n",
    "                                'lift': lift\n",
    "                            })\n",
    "\n",
    "        return rules\n",
    "\n",
    "    def _generate_subsets(self, itemset: FrozenSet, size: int) -> List[FrozenSet]:\n",
    "        \"\"\"Generate all subsets of a given size from an itemset.\"\"\"\n",
    "        from itertools import combinations\n",
    "        return [frozenset(combo) for combo in combinations(itemset, size)]\n",
    "\n",
    "    def _get_itemset_support(self, itemset: FrozenSet) -> float:\n",
    "        \"\"\"Get the support of an itemset.\"\"\"\n",
    "        # For closed itemsets, we already have the support\n",
    "        for closed_set, support in self.closed_itemsets:\n",
    "            if itemset == closed_set:\n",
    "                return support\n",
    "\n",
    "        # For non-closed itemsets, find the smallest closed superset\n",
    "        best_support = None\n",
    "        for closed_set, support in self.closed_itemsets:\n",
    "            if itemset.issubset(closed_set):\n",
    "                if best_support is None or len(closed_set) < best_support[0]:\n",
    "                    best_support = (len(closed_set), support)\n",
    "\n",
    "        # If no superset found (shouldn't happen if properly computed)\n",
    "        if best_support is None:\n",
    "            # Fall back to direct calculation from transaction data\n",
    "            mask = self.transaction_df[list(itemset)].all(axis=1)\n",
    "            return mask.sum() / len(self.transaction_df)\n",
    "\n",
    "        return best_support[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7c356c",
   "metadata": {},
   "source": [
    "## Compute Closed Itemsets\n",
    "\n",
    "Let's compute closed itemsets using our implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1241d957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the concept lattice\n",
    "start_time = time.time()\n",
    "concept_lattice = ConceptLattice(encoded_df)\n",
    "closed_itemsets = concept_lattice.compute_closed_itemsets()\n",
    "lattice_time = time.time() - start_time\n",
    "\n",
    "print(f\"Computed {len(closed_itemsets)} closed itemsets in {lattice_time:.2f} seconds\")\n",
    "\n",
    "# Display top closed itemsets by support\n",
    "print(\"\\nTop 10 Closed Itemsets by Support:\")\n",
    "for i, (itemset, support) in enumerate(sorted(closed_itemsets, key=lambda x: x[1], reverse=True)[:10]):\n",
    "    print(f\"{i+1}. Items: {sorted(itemset)}, Support: {support:.2f}\")\n",
    "\n",
    "# Save closed itemsets to file\n",
    "closed_itemsets_data = [{\n",
    "    'itemset': sorted(list(itemset)),\n",
    "    'support': support\n",
    "} for itemset, support in closed_itemsets]\n",
    "closed_itemsets_path = 'output/closed_itemsets.json'\n",
    "save_json_file(closed_itemsets_data, closed_itemsets_path)\n",
    "print(f\"\\nSaved closed itemsets to {closed_itemsets_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645e8fb2",
   "metadata": {},
   "source": [
    "## Build and Visualize Concept Lattice\n",
    "\n",
    "Now, let's build the concept lattice and visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cd584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the lattice\n",
    "lattice = concept_lattice.build_lattice()\n",
    "print(f\"Built concept lattice with {lattice.number_of_nodes()} nodes and {lattice.number_of_edges()} edges\")\n",
    "\n",
    "# Analyze the lattice structure\n",
    "print(\"\\nLattice Structure Analysis:\")\n",
    "print(f\"  Number of nodes (concepts): {lattice.number_of_nodes()}\")\n",
    "print(f\"  Number of edges (cover relations): {lattice.number_of_edges()}\")\n",
    "\n",
    "# Find nodes with highest in-degree and out-degree\n",
    "max_in_degree = 0\n",
    "max_in_node = None\n",
    "max_out_degree = 0\n",
    "max_out_node = None\n",
    "\n",
    "for node in lattice.nodes():\n",
    "    in_degree = lattice.in_degree(node)\n",
    "    out_degree = lattice.out_degree(node)\n",
    "\n",
    "    if in_degree > max_in_degree:\n",
    "        max_in_degree = in_degree\n",
    "        max_in_node = node\n",
    "\n",
    "    if out_degree > max_out_degree:\n",
    "        max_out_degree = out_degree\n",
    "        max_out_node = node\n",
    "\n",
    "# Get node data for these special nodes\n",
    "node_data_in = lattice.nodes[max_in_node] if max_in_node is not None else None\n",
    "node_data_out = lattice.nodes[max_out_node] if max_out_node is not None else None\n",
    "\n",
    "print(f\"  Node with most parents (max in-degree: {max_in_degree}):\")\n",
    "print(f\"    {node_data_in['label'] if node_data_in else 'None'}\")\n",
    "\n",
    "print(f\"  Node with most children (max out-degree: {max_out_degree}):\")\n",
    "print(f\"    {node_data_out['label'] if node_data_out else 'None'}\")\n",
    "\n",
    "# Visualize the lattice\n",
    "print(\"\\nVisualizing concept lattice...\")\n",
    "concept_lattice.visualize_lattice(\n",
    "    output_path='figures/lattice.png',\n",
    "    node_size=1200,\n",
    "    font_size=8,\n",
    "    edge_width=1.5\n",
    ")\n",
    "print(\"Lattice visualization saved to 'figures/lattice.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ed0234",
   "metadata": {},
   "source": [
    "## Compare with Frequent Itemsets from Apriori/FP-Growth\n",
    "\n",
    "Let's compare our closed itemsets with the frequent itemsets found by Apriori and FP-Growth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6100e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load frequent itemsets from Apriori\n",
    "try:\n",
    "    apriori_itemsets_df = pd.read_csv('output/apriori_itemsets.csv')\n",
    "    print(f\"Loaded {len(apriori_itemsets_df)} Apriori frequent itemsets\")\n",
    "except FileNotFoundError:\n",
    "    # Compute Apriori itemsets if not found\n",
    "    print(\"Apriori itemsets file not found. Computing...\")\n",
    "    apriori_itemsets = apriori(encoded_df, min_support=min_support, use_colnames=True)\n",
    "    apriori_itemsets_df = frozenset_to_list(apriori_itemsets)\n",
    "    print(f\"Computed {len(apriori_itemsets_df)} Apriori frequent itemsets\")\n",
    "\n",
    "# Count closed vs. frequent itemsets\n",
    "print(f\"\\nComparison of Itemset Counts:\")\n",
    "print(f\"  Apriori frequent itemsets: {len(apriori_itemsets_df)}\")\n",
    "print(f\"  Closed itemsets: {len(closed_itemsets)}\")\n",
    "print(f\"  Reduction ratio: {len(closed_itemsets) / len(apriori_itemsets_df):.2f}\")\n",
    "\n",
    "# Convert closed itemsets to a format comparable with Apriori itemsets\n",
    "closed_itemsets_list = []\n",
    "for itemset, support in closed_itemsets:\n",
    "    closed_itemsets_list.append({\n",
    "        'itemsets': sorted(list(itemset)),\n",
    "        'support': support\n",
    "    })\n",
    "\n",
    "closed_df = pd.DataFrame(closed_itemsets_list)\n",
    "\n",
    "# Compare the distributions of itemset sizes\n",
    "if 'itemsets' in apriori_itemsets_df.columns:\n",
    "    apriori_sizes = apriori_itemsets_df['itemsets'].apply(lambda x: len(eval(x)) if isinstance(x, str) else len(x))\n",
    "    closed_sizes = closed_df['itemsets'].apply(len)\n",
    "\n",
    "    # Create DataFrames for plotting\n",
    "    apriori_size_df = pd.DataFrame({'Size': apriori_sizes, 'Type': 'Apriori'})\n",
    "    closed_size_df = pd.DataFrame({'Size': closed_sizes, 'Type': 'Closed'})\n",
    "    combined_df = pd.concat([apriori_size_df, closed_size_df])\n",
    "\n",
    "    # Plot size distributions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(x='Size', hue='Type', data=combined_df)\n",
    "    plt.title('Distribution of Itemset Sizes: Apriori vs Closed')\n",
    "    plt.xlabel('Itemset Size')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.legend(title='Itemset Type')\n",
    "\n",
    "    # Save the figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/itemset_comparison.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b87260",
   "metadata": {},
   "source": [
    "## Generate Concise Rules from Closed Itemsets\n",
    "\n",
    "Let's generate concise rules from closed itemsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9890b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate concise rules from closed itemsets\n",
    "start_time = time.time()\n",
    "closed_rules = concept_lattice.generate_concise_rules(min_confidence=min_confidence)\n",
    "rule_generation_time = time.time() - start_time\n",
    "\n",
    "print(f\"Generated {len(closed_rules)} concise rules from closed itemsets in {rule_generation_time:.2f} seconds\")\n",
    "\n",
    "# Save rules to file\n",
    "closed_rules_path = 'output/closed_rules.json'\n",
    "save_json_file(closed_rules, closed_rules_path)\n",
    "print(f\"Saved closed rules to {closed_rules_path}\")\n",
    "\n",
    "# Display top rules by lift\n",
    "print(\"\\nTop 10 Concise Rules by Lift:\")\n",
    "sorted_rules = sorted(closed_rules, key=lambda x: x['lift'], reverse=True)\n",
    "for i, rule in enumerate(sorted_rules[:10]):\n",
    "    print(f\"{i+1}. {rule['antecedent']} → {rule['consequent']} \" +\n",
    "          f\"(Support: {rule['support']:.2f}, Confidence: {rule['confidence']:.2f}, Lift: {rule['lift']:.2f})\")\n",
    "\n",
    "# Convert to DataFrame for further analysis\n",
    "closed_rules_df = pd.DataFrame(closed_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dce0d4",
   "metadata": {},
   "source": [
    "## Compare with Association Rules from Apriori/FP-Growth\n",
    "\n",
    "Let's compare our concise rules with association rules from traditional algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5f6a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load association rules from Apriori\n",
    "try:\n",
    "    apriori_rules_df = pd.read_csv('output/apriori_rules.csv')\n",
    "    print(f\"Loaded {len(apriori_rules_df)} Apriori association rules\")\n",
    "except FileNotFoundError:\n",
    "    # Generate Apriori rules if not found\n",
    "    print(\"Apriori rules file not found.\")\n",
    "    if 'apriori_itemsets' in locals():\n",
    "        apriori_rules = association_rules(apriori_itemsets, metric=\"confidence\", min_threshold=min_confidence)\n",
    "        apriori_rules_df = frozenset_to_list(apriori_rules)\n",
    "        print(f\"Generated {len(apriori_rules_df)} Apriori association rules\")\n",
    "    else:\n",
    "        print(\"Cannot generate Apriori rules without itemsets.\")\n",
    "        apriori_rules_df = pd.DataFrame()\n",
    "\n",
    "# Compare rule counts\n",
    "print(f\"\\nComparison of Rule Counts:\")\n",
    "print(f\"  Apriori association rules: {len(apriori_rules_df)}\")\n",
    "print(f\"  Concise rules from closed itemsets: {len(closed_rules_df)}\")\n",
    "\n",
    "if len(apriori_rules_df) > 0:\n",
    "    print(f\"  Reduction ratio: {len(closed_rules_df) / len(apriori_rules_df):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1c64c6",
   "metadata": {},
   "source": [
    "## Visualize Rule Metrics\n",
    "\n",
    "Let's visualize the metrics of our closed-based rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534062b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rule metrics distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Support distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(closed_rules_df['support'], kde=True)\n",
    "plt.title('Support Distribution')\n",
    "plt.xlabel('Support')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Confidence distribution\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(closed_rules_df['confidence'], kde=True)\n",
    "plt.title('Confidence Distribution')\n",
    "plt.xlabel('Confidence')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Lift distribution\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(closed_rules_df['lift'], kde=True)\n",
    "plt.title('Lift Distribution')\n",
    "plt.xlabel('Lift')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Save the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/closed_rule_metrics.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5870b9a6",
   "metadata": {},
   "source": [
    "## Create Rule Graph\n",
    "\n",
    "Let's create a graph of the strongest rules for visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0407aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rule_graph(\n",
    "    rules_df: pd.DataFrame,\n",
    "    min_lift: float = 1.5,\n",
    "    top_n: int = 20,\n",
    "    output_path: str = 'figures/rule_graph.png'\n",
    ") -> None:\n",
    "    \"\"\"Create a graph visualization of top association rules.\"\"\"\n",
    "    # Filter rules by lift and take top N by lift\n",
    "    if len(rules_df) == 0:\n",
    "        print(\"No rules to visualize.\")\n",
    "        return\n",
    "\n",
    "    if isinstance(rules_df['lift'].iloc[0], str):\n",
    "        rules_df['lift'] = rules_df['lift'].astype(float)\n",
    "\n",
    "    filtered_rules = rules_df[rules_df['lift'] >= min_lift].sort_values('lift', ascending=False)\n",
    "    top_rules = filtered_rules.head(top_n)\n",
    "\n",
    "    # Create graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add edges for each rule\n",
    "    for _, rule in top_rules.iterrows():\n",
    "        antecedent = rule['antecedent']\n",
    "        consequent = rule['consequent']\n",
    "\n",
    "        # Convert to list if needed\n",
    "        if isinstance(antecedent, str):\n",
    "            antecedent = eval(antecedent)\n",
    "        if isinstance(consequent, str):\n",
    "            consequent = eval(consequent)\n",
    "\n",
    "        for item_from in antecedent:\n",
    "            for item_to in consequent:\n",
    "                if G.has_edge(item_from, item_to):\n",
    "                    # Increase edge weight if rule already exists\n",
    "                    G[item_from][item_to]['weight'] += float(rule['lift'])\n",
    "                else:\n",
    "                    # Add new edge with lift as weight\n",
    "                    G.add_edge(\n",
    "                        item_from,\n",
    "                        item_to,\n",
    "                        weight=float(rule['lift']),\n",
    "                        confidence=float(rule['confidence']),\n",
    "                        support=float(rule['support'])\n",
    "                    )\n",
    "\n",
    "    # Skip if no edges\n",
    "    if len(G.edges()) == 0:\n",
    "        print(\"No edges to visualize.\")\n",
    "        return\n",
    "\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(14, 10))\n",
    "\n",
    "    # Compute layout\n",
    "    pos = nx.spring_layout(G, k=2, seed=42)\n",
    "\n",
    "    # Get edge weights for width and color\n",
    "    edges = G.edges()\n",
    "    edge_weights = [G[u][v]['weight'] for u, v in edges]\n",
    "\n",
    "    # Normalize edge weights for width\n",
    "    max_weight = max(edge_weights)\n",
    "    min_weight = min(edge_weights)\n",
    "    norm_range = max_weight - min_weight\n",
    "    normalized_weights = [\n",
    "        1 + 8 * (w - min_weight) / norm_range if norm_range > 0 else 1\n",
    "        for w in edge_weights\n",
    "    ]\n",
    "\n",
    "    # Draw the graph\n",
    "    nx.draw_networkx(\n",
    "        G,\n",
    "        pos=pos,\n",
    "        node_size=2000,\n",
    "        font_size=10,\n",
    "        font_weight='bold',\n",
    "        width=normalized_weights,\n",
    "        edge_color=edge_weights,\n",
    "        edge_cmap=plt.cm.YlOrRd,\n",
    "        node_color='lightblue',\n",
    "        with_labels=True,\n",
    "        arrows=True,\n",
    "        arrowsize=20,\n",
    "        connectionstyle='arc3,rad=0.2'\n",
    "    )\n",
    "\n",
    "    # Add a colorbar for edge weights (lift)\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.YlOrRd, norm=plt.Normalize(min_weight, max_weight))\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm)\n",
    "    cbar.set_label('Lift', fontsize=12)\n",
    "\n",
    "    plt.title('Association Rules Network (Items & Strong Rules)', fontsize=16)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Save figure\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Create rule graph\n",
    "plot_rule_graph(\n",
    "    closed_rules_df,\n",
    "    min_lift=1.5,\n",
    "    top_n=20,\n",
    "    output_path='figures/closed_rule_graph.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847cdee8",
   "metadata": {},
   "source": [
    "## Lattice Pruning\n",
    "\n",
    "Let's examine how pruning affects the concept lattice by varying the support threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d763fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different support thresholds\n",
    "support_thresholds = [0.1, 0.2, 0.3]\n",
    "pruning_results = {}\n",
    "\n",
    "for threshold in support_thresholds:\n",
    "    # Filter closed itemsets by support\n",
    "    pruned_itemsets = [(itemset, support) for itemset, support in closed_itemsets if support >= threshold]\n",
    "\n",
    "    pruning_results[threshold] = {\n",
    "        'itemset_count': len(pruned_itemsets),\n",
    "        'max_size': max([len(itemset) for itemset, _ in pruned_itemsets]) if pruned_itemsets else 0,\n",
    "        'min_support': threshold\n",
    "    }\n",
    "\n",
    "    print(f\"Support threshold {threshold}: {len(pruned_itemsets)} closed itemsets\")\n",
    "\n",
    "# Plot pruning results\n",
    "plt.figure(figsize=(10, 6))\n",
    "thresholds = list(pruning_results.keys())\n",
    "counts = [pruning_results[t]['itemset_count'] for t in thresholds]\n",
    "\n",
    "plt.plot(thresholds, counts, marker='o', linestyle='-', linewidth=2)\n",
    "plt.title('Effect of Support Threshold on Concept Lattice Size')\n",
    "plt.xlabel('Minimum Support Threshold')\n",
    "plt.ylabel('Number of Closed Itemsets')\n",
    "plt.grid(True)\n",
    "plt.xticks(thresholds)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(counts):\n",
    "    plt.text(thresholds[i], v + 0.5, str(v), ha='center')\n",
    "\n",
    "# Save the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/lattice_pruning.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c06af0",
   "metadata": {},
   "source": [
    "## Relationship Between Closed Itemsets and Generator Sets\n",
    "\n",
    "Explore the relationship between closed itemsets and their minimal generators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93302df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_minimal_generators(closed_itemsets, transaction_df):\n",
    "    \"\"\"Find minimal generators for closed itemsets.\"\"\"\n",
    "    generators_map = {}\n",
    "\n",
    "    # Convert transaction_df to list of sets for easier processing\n",
    "    transactions = []\n",
    "    for _, row in transaction_df.iterrows():\n",
    "        transaction = frozenset([\n",
    "            col for col, val in row.items() if val\n",
    "        ])\n",
    "        transactions.append(transaction)\n",
    "\n",
    "    # For each closed itemset\n",
    "    for closed_set, support in closed_itemsets:\n",
    "        if not closed_set:  # Skip empty set\n",
    "            continue\n",
    "\n",
    "        # Find minimal generators for this closed itemset\n",
    "        generators = []\n",
    "\n",
    "        # Get the support count for this closed itemset\n",
    "        closed_support_count = int(support * len(transactions))\n",
    "\n",
    "        # Try subsets of the closed itemset\n",
    "        for size in range(1, len(closed_set) + 1):\n",
    "            for subset in combinations(closed_set, size):\n",
    "                subset = frozenset(subset)\n",
    "\n",
    "                # Check if subset has same support as closed_set\n",
    "                subset_count = sum(1 for t in transactions if subset.issubset(t))\n",
    "                if subset_count == closed_support_count:\n",
    "                    # Check if it's minimal\n",
    "                    is_minimal = True\n",
    "                    for gen in generators:\n",
    "                        if gen.issubset(subset):\n",
    "                            is_minimal = False\n",
    "                            break\n",
    "\n",
    "                    if is_minimal:\n",
    "                        # Remove any existing generators that are supersets\n",
    "                        generators = [g for g in generators if not subset.issubset(g)]\n",
    "                        generators.append(subset)\n",
    "\n",
    "        generators_map[frozenset(closed_set)] = generators\n",
    "\n",
    "    return generators_map\n",
    "\n",
    "# Find minimal generators for a subset of closed itemsets (this can be computationally expensive)\n",
    "sample_size = min(10, len(closed_itemsets))\n",
    "sample_closed_itemsets = closed_itemsets[:sample_size]\n",
    "\n",
    "print(f\"Finding minimal generators for {sample_size} closed itemsets...\")\n",
    "generators = find_minimal_generators(sample_closed_itemsets, encoded_df)\n",
    "\n",
    "print(\"\\nClosed Itemsets and their Minimal Generators:\")\n",
    "for closed_set, gens in generators.items():\n",
    "    if gens:  # Skip empty generators\n",
    "        print(f\"Closed Set: {sorted(closed_set)}\")\n",
    "        for i, gen in enumerate(gens):\n",
    "            print(f\"  Generator {i+1}: {sorted(gen)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2623166",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "1. Implemented a concept lattice based on formal concept analysis\n",
    "2. Computed closed itemsets using the closure operator\n",
    "3. Built and visualized the concept lattice (Hasse diagram)\n",
    "4. Generated concise rules from closed itemsets\n",
    "5. Compared closed itemsets with traditional frequent itemsets\n",
    "6. Demonstrated lattice pruning by varying support threshold\n",
    "7. Explored the relationship between closed itemsets and generators\n",
    "\n",
    "Key insights:\n",
    "- Closed itemsets provide a more concise representation of patterns in the data\n",
    "- The concept lattice reveals the hierarchical structure of closed itemsets\n",
    "- Concise rules from closed itemsets can be more informative than traditional association rules\n",
    "- Lattice pruning allows us to focus on the most significant patterns\n",
    "\n",
    "### Theoretical Complexity Analysis\n",
    "\n",
    "- **Computing Closed Itemsets**:\n",
    "  - Time Complexity: O(n * 2^d) in the worst case, where n is the number of transactions and d is the number of unique items\n",
    "  - Space Complexity: O(n * c), where c is the number of closed itemsets\n",
    "\n",
    "- **Building Concept Lattice**:\n",
    "  - Time Complexity: O(c^2 * d), where c is the number of closed itemsets and d is the maximum itemset size\n",
    "  - Space Complexity: O(c + e), where e is the number of edges in the lattice\n",
    "\n",
    "- **Generating Rules from Closed Itemsets**:\n",
    "  - Time Complexity: O(c * 2^d_max), where d_max is the maximum closed itemset size\n",
    "  - Space Complexity: O(r), where r is the number of rules generated\n",
    "\n",
    "In the next notebook, we'll focus on evaluating the quality of the rules we've discovered and conduct experiments to compare the different approaches."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
