{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "808bae6a",
   "metadata": {},
   "source": [
    "# 4. Evaluation and Experiments\n",
    "\n",
    "This notebook focuses on evaluating the quality of the patterns and rules discovered in our Frequent Pattern Mining project. We'll compare traditional approaches with our concept lattice-based approach and perform various experiments to assess their strengths and weaknesses.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Load rules from both traditional and concept lattice approaches\n",
    "- Compare rule quality using various metrics\n",
    "- Evaluate rule interestingness and diversity\n",
    "- Perform sensitivity analysis with different parameters\n",
    "- Benchmark performance (time and memory)\n",
    "- Visualize comparative results\n",
    "- Recommend the best approach based on use case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df1dc5e",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8e4e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Set, Tuple, Any, Union\n",
    "import networkx as nx\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import memory_profiler\n",
    "import warnings\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Ignore FutureWarnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs('output', exist_ok=True)\n",
    "os.makedirs('figures', exist_ok=True)\n",
    "os.makedirs('evaluation', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837c6bd4",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d17c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_file(file_path: str) -> Any:\n",
    "    \"\"\"Load data from a JSON file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def save_json_file(data: Any, file_path: str) -> None:\n",
    "    \"\"\"Save data to a JSON file.\"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "\n",
    "def load_rules(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load rules from a file (CSV or JSON) and return as DataFrame.\"\"\"\n",
    "    if file_path.endswith('.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Convert string representations of lists to actual lists if needed\n",
    "        for col in ['antecedent', 'consequent']:\n",
    "            if col in df.columns and isinstance(df[col].iloc[0], str):\n",
    "                df[col] = df[col].apply(eval)\n",
    "\n",
    "        return df\n",
    "\n",
    "    elif file_path.endswith('.json'):\n",
    "        data = load_json_file(file_path)\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_path}\")\n",
    "\n",
    "\n",
    "def frozenset_to_list(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert frozenset columns in a DataFrame to lists for easier handling.\"\"\"\n",
    "    result = df.copy()\n",
    "\n",
    "    # Convert any frozenset columns to lists\n",
    "    for col in result.columns:\n",
    "        if result[col].apply(lambda x: isinstance(x, frozenset)).any():\n",
    "            result[col] = result[col].apply(lambda x: list(x) if isinstance(x, frozenset) else x)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def evaluate_rule_metrics(rules_df: pd.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"Calculate various evaluation metrics for rules.\"\"\"\n",
    "    metrics = {}\n",
    "\n",
    "    # Basic metrics\n",
    "    metrics['rule_count'] = len(rules_df)\n",
    "    metrics['avg_support'] = rules_df['support'].mean()\n",
    "    metrics['avg_confidence'] = rules_df['confidence'].mean()\n",
    "    metrics['avg_lift'] = rules_df['lift'].mean()\n",
    "\n",
    "    # Itemset size metrics\n",
    "    if 'antecedent' in rules_df.columns and isinstance(rules_df['antecedent'].iloc[0], (list, set, frozenset)):\n",
    "        rules_df['antecedent_size'] = rules_df['antecedent'].apply(len)\n",
    "        rules_df['consequent_size'] = rules_df['consequent'].apply(len)\n",
    "        rules_df['rule_size'] = rules_df['antecedent_size'] + rules_df['consequent_size']\n",
    "\n",
    "        metrics['avg_antecedent_size'] = rules_df['antecedent_size'].mean()\n",
    "        metrics['avg_consequent_size'] = rules_df['consequent_size'].mean()\n",
    "        metrics['avg_rule_size'] = rules_df['rule_size'].mean()\n",
    "\n",
    "    # Rule quality metrics\n",
    "    if 'lift' in rules_df.columns:\n",
    "        metrics['high_lift_ratio'] = (rules_df['lift'] > 1).mean()\n",
    "        metrics['very_high_lift_ratio'] = (rules_df['lift'] > 3).mean()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def calculate_rule_diversity(rules_df: pd.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"Calculate diversity metrics for a set of rules.\"\"\"\n",
    "    diversity = {}\n",
    "\n",
    "    # Get unique items across all rules\n",
    "    unique_items = set()\n",
    "    for _, row in rules_df.iterrows():\n",
    "        if 'antecedent' in rules_df.columns and 'consequent' in rules_df.columns:\n",
    "            antecedent = row['antecedent']\n",
    "            consequent = row['consequent']\n",
    "\n",
    "            # Convert to list if they are strings\n",
    "            if isinstance(antecedent, str):\n",
    "                antecedent = eval(antecedent)\n",
    "            if isinstance(consequent, str):\n",
    "                consequent = eval(consequent)\n",
    "\n",
    "            unique_items.update(antecedent)\n",
    "            unique_items.update(consequent)\n",
    "\n",
    "    diversity['unique_item_count'] = len(unique_items)\n",
    "\n",
    "    # Calculate coverage of items\n",
    "    if 'support' in rules_df.columns:\n",
    "        item_support = defaultdict(float)\n",
    "        for _, row in rules_df.iterrows():\n",
    "            support = row['support']\n",
    "\n",
    "            antecedent = row['antecedent']\n",
    "            consequent = row['consequent']\n",
    "\n",
    "            # Convert to list if they are strings\n",
    "            if isinstance(antecedent, str):\n",
    "                antecedent = eval(antecedent)\n",
    "            if isinstance(consequent, str):\n",
    "                consequent = eval(consequent)\n",
    "\n",
    "            for item in list(antecedent) + list(consequent):\n",
    "                item_support[item] += support\n",
    "\n",
    "        diversity['item_coverage'] = sum(item_support.values()) / len(item_support) if item_support else 0\n",
    "\n",
    "    # Calculate rule similarity - Jaccard similarity between rules\n",
    "    if len(rules_df) > 1:\n",
    "        similarities = []\n",
    "        for i in range(len(rules_df)):\n",
    "            for j in range(i+1, len(rules_df)):\n",
    "                rule1 = set(rules_df.iloc[i]['antecedent'] + rules_df.iloc[i]['consequent'])\n",
    "                rule2 = set(rules_df.iloc[j]['antecedent'] + rules_df.iloc[j]['consequent'])\n",
    "\n",
    "                # Convert to set if they are strings\n",
    "                if isinstance(rule1, str):\n",
    "                    rule1 = set(eval(rule1))\n",
    "                if isinstance(rule2, str):\n",
    "                    rule2 = set(eval(rule2))\n",
    "\n",
    "                # Calculate Jaccard similarity\n",
    "                intersection = len(rule1.intersection(rule2))\n",
    "                union = len(rule1.union(rule2))\n",
    "                similarity = intersection / union if union > 0 else 0\n",
    "                similarities.append(similarity)\n",
    "\n",
    "        diversity['avg_rule_similarity'] = sum(similarities) / len(similarities) if similarities else 0\n",
    "        diversity['rule_diversity'] = 1 - diversity['avg_rule_similarity']\n",
    "    else:\n",
    "        diversity['avg_rule_similarity'] = 0\n",
    "        diversity['rule_diversity'] = 1\n",
    "\n",
    "    return diversity\n",
    "\n",
    "\n",
    "def calculate_redundancy(rules_df: pd.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"Calculate redundancy metrics for a set of rules.\"\"\"\n",
    "    redundancy = {}\n",
    "\n",
    "    if len(rules_df) <= 1:\n",
    "        redundancy['redundancy_ratio'] = 0\n",
    "        return redundancy\n",
    "\n",
    "    # Create sets for rules (antecedent -> consequent)\n",
    "    rule_implications = []\n",
    "    for _, row in rules_df.iterrows():\n",
    "        antecedent = row['antecedent']\n",
    "        consequent = row['consequent']\n",
    "\n",
    "        # Convert to frozenset if they are strings or lists\n",
    "        if isinstance(antecedent, str):\n",
    "            antecedent = eval(antecedent)\n",
    "        if isinstance(consequent, str):\n",
    "            consequent = eval(consequent)\n",
    "\n",
    "        if isinstance(antecedent, list):\n",
    "            antecedent = frozenset(antecedent)\n",
    "        if isinstance(consequent, list):\n",
    "            consequent = frozenset(consequent)\n",
    "\n",
    "        rule_implications.append((antecedent, consequent))\n",
    "\n",
    "    # Count redundant rules (rules that can be derived from others)\n",
    "    redundant_count = 0\n",
    "\n",
    "    for i, (ant_i, cons_i) in enumerate(rule_implications):\n",
    "        for j, (ant_j, cons_j) in enumerate(rule_implications):\n",
    "            if i == j:\n",
    "                continue\n",
    "\n",
    "            # If ant_i ⊆ ant_j and cons_i ⊇ cons_j, then rule i is redundant\n",
    "            if ant_i.issubset(ant_j) and cons_i.issuperset(cons_j):\n",
    "                redundant_count += 1\n",
    "                break\n",
    "\n",
    "    redundancy['redundancy_ratio'] = redundant_count / len(rules_df)\n",
    "    return redundancy\n",
    "\n",
    "\n",
    "@memory_profiler.profile\n",
    "def measure_memory_usage(func, *args, **kwargs):\n",
    "    \"\"\"Measure memory usage of a function.\"\"\"\n",
    "    return func(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1fe4ca",
   "metadata": {},
   "source": [
    "## Load Rules and Itemsets from Previous Steps\n",
    "\n",
    "Let's load the rules from both traditional algorithms and our concept lattice approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f09aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "try:\n",
    "    config = load_json_file('config.json')\n",
    "    min_support = config.get('min_support', 0.2)\n",
    "    min_confidence = config.get('min_confidence', 0.6)\n",
    "except (FileNotFoundError, json.JSONDecodeError):\n",
    "    print(\"Config file not found or invalid. Using default parameters.\")\n",
    "    min_support = 0.2\n",
    "    min_confidence = 0.6\n",
    "\n",
    "print(f\"Parameters:\")\n",
    "print(f\"  Min Support: {min_support}\")\n",
    "print(f\"  Min Confidence: {min_confidence}\")\n",
    "\n",
    "# Load rules\n",
    "rules_data = {}\n",
    "\n",
    "# Load closed rules\n",
    "try:\n",
    "    closed_rules = load_json_file('output/closed_rules.json')\n",
    "    closed_rules_df = pd.DataFrame(closed_rules)\n",
    "    rules_data['closed'] = closed_rules_df\n",
    "    print(f\"\\nLoaded {len(closed_rules_df)} rules from closed itemsets\")\n",
    "except (FileNotFoundError, json.JSONDecodeError):\n",
    "    print(\"Closed rules file not found or invalid.\")\n",
    "    closed_rules_df = pd.DataFrame()\n",
    "\n",
    "# Load Apriori rules\n",
    "try:\n",
    "    apriori_rules_df = load_rules('output/apriori_rules.csv')\n",
    "    rules_data['apriori'] = apriori_rules_df\n",
    "    print(f\"Loaded {len(apriori_rules_df)} rules from Apriori algorithm\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Apriori rules file not found.\")\n",
    "    apriori_rules_df = pd.DataFrame()\n",
    "\n",
    "# Load FP-Growth rules\n",
    "try:\n",
    "    fpgrowth_rules_df = load_rules('output/fpgrowth_rules.csv')\n",
    "    rules_data['fpgrowth'] = fpgrowth_rules_df\n",
    "    print(f\"Loaded {len(fpgrowth_rules_df)} rules from FP-Growth algorithm\")\n",
    "except FileNotFoundError:\n",
    "    print(\"FP-Growth rules file not found.\")\n",
    "    fpgrowth_rules_df = pd.DataFrame()\n",
    "\n",
    "# Load itemsets\n",
    "itemsets_data = {}\n",
    "\n",
    "# Load closed itemsets\n",
    "try:\n",
    "    closed_itemsets = load_json_file('output/closed_itemsets.json')\n",
    "    itemsets_data['closed'] = closed_itemsets\n",
    "    print(f\"\\nLoaded {len(closed_itemsets)} closed itemsets\")\n",
    "except (FileNotFoundError, json.JSONDecodeError):\n",
    "    print(\"Closed itemsets file not found or invalid.\")\n",
    "    closed_itemsets = []\n",
    "\n",
    "# Load Apriori itemsets\n",
    "try:\n",
    "    apriori_itemsets_df = pd.read_csv('output/apriori_itemsets.csv')\n",
    "    itemsets_data['apriori'] = apriori_itemsets_df\n",
    "    print(f\"Loaded {len(apriori_itemsets_df)} Apriori frequent itemsets\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Apriori itemsets file not found.\")\n",
    "    apriori_itemsets_df = pd.DataFrame()\n",
    "\n",
    "# Load FP-Growth itemsets\n",
    "try:\n",
    "    fpgrowth_itemsets_df = pd.read_csv('output/fpgrowth_itemsets.csv')\n",
    "    itemsets_data['fpgrowth'] = fpgrowth_itemsets_df\n",
    "    print(f\"Loaded {len(fpgrowth_itemsets_df)} FP-Growth frequent itemsets\")\n",
    "except FileNotFoundError:\n",
    "    print(\"FP-Growth itemsets file not found.\")\n",
    "    fpgrowth_itemsets_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4610599",
   "metadata": {},
   "source": [
    "## Load Transaction Data\n",
    "\n",
    "Let's load the transaction data for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f3cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load encoded transaction data\n",
    "encoded_path = 'output/encoded_transactions.csv'\n",
    "\n",
    "try:\n",
    "    encoded_df = pd.read_csv(encoded_path)\n",
    "    print(f\"Loaded encoded transactions: {encoded_df.shape}\")\n",
    "\n",
    "    # Display a sample of the encoded data\n",
    "    print(\"\\nSample of Encoded Transactions:\")\n",
    "    print(encoded_df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Encoded transactions file not found at {encoded_path}\")\n",
    "    print(\"Creating a sample encoded dataset...\")\n",
    "\n",
    "    # Load transaction lists\n",
    "    try:\n",
    "        transaction_lists = load_json_file('output/transaction_lists.json')\n",
    "    except FileNotFoundError:\n",
    "        # Create sample transactions if needed\n",
    "        transaction_lists = [\n",
    "            [\"apple\", \"milk\", \"bread\"],\n",
    "            [\"rice\", \"oil\"],\n",
    "            [\"milk\", \"eggs\", \"cheese\"],\n",
    "            [\"bread\", \"butter\", \"milk\"],\n",
    "            [\"apple\", \"banana\", \"orange\"]\n",
    "        ]\n",
    "\n",
    "    # Encode the transactions\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit_transform(transaction_lists)\n",
    "    encoded_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "    print(f\"Created sample encoded transactions: {encoded_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a1236f",
   "metadata": {},
   "source": [
    "## Calculate Evaluation Metrics\n",
    "\n",
    "Let's calculate various metrics to evaluate the quality of rules from each approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51601c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for each rule set\n",
    "metrics = {}\n",
    "diversity = {}\n",
    "redundancy = {}\n",
    "\n",
    "for method, rules_df in rules_data.items():\n",
    "    if len(rules_df) > 0:\n",
    "        print(f\"\\nCalculating metrics for {method} rules...\")\n",
    "        metrics[method] = evaluate_rule_metrics(rules_df)\n",
    "        diversity[method] = calculate_rule_diversity(rules_df)\n",
    "        redundancy[method] = calculate_redundancy(rules_df)\n",
    "\n",
    "# Display metrics\n",
    "print(\"\\nRule Metrics:\")\n",
    "for method, method_metrics in metrics.items():\n",
    "    print(f\"\\n{method.capitalize()} Rules:\")\n",
    "    for metric, value in method_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nDiversity Metrics:\")\n",
    "for method, method_diversity in diversity.items():\n",
    "    print(f\"\\n{method.capitalize()} Rules:\")\n",
    "    for metric, value in method_diversity.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nRedundancy Metrics:\")\n",
    "for method, method_redundancy in redundancy.items():\n",
    "    print(f\"\\n{method.capitalize()} Rules:\")\n",
    "    for metric, value in method_redundancy.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Save metrics to file\n",
    "evaluation_data = {\n",
    "    'metrics': metrics,\n",
    "    'diversity': diversity,\n",
    "    'redundancy': redundancy\n",
    "}\n",
    "\n",
    "save_json_file(evaluation_data, 'evaluation/metrics.json')\n",
    "print(\"\\nSaved evaluation metrics to 'evaluation/metrics.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b29aa71",
   "metadata": {},
   "source": [
    "## Visualize Comparison of Metrics\n",
    "\n",
    "Let's create visualizations to compare the different approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f50012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_comparison(\n",
    "    metrics_dict: Dict[str, Dict[str, float]],\n",
    "    metric_name: str,\n",
    "    title: str,\n",
    "    ylabel: str,\n",
    "    output_path: str,\n",
    "    higher_is_better: bool = True,\n",
    "    format_func=lambda x: f\"{x:.2f}\"\n",
    "):\n",
    "    \"\"\"Plot a comparison of a specific metric across different methods.\"\"\"\n",
    "    # Extract data\n",
    "    methods = []\n",
    "    values = []\n",
    "\n",
    "    for method, method_metrics in metrics_dict.items():\n",
    "        if metric_name in method_metrics:\n",
    "            methods.append(method.capitalize())\n",
    "            values.append(method_metrics[metric_name])\n",
    "\n",
    "    if not methods:\n",
    "        print(f\"No data available for metric: {metric_name}\")\n",
    "        return\n",
    "\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Define color based on whether higher is better\n",
    "    colors = ['#2ecc71' if higher_is_better else '#e74c3c'] * len(methods)\n",
    "\n",
    "    # Create bar chart\n",
    "    bars = plt.bar(methods, values, color=colors, alpha=0.8)\n",
    "\n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width()/2.,\n",
    "            height * 1.01,\n",
    "            format_func(height),\n",
    "            ha='center', va='bottom',\n",
    "            fontweight='bold'\n",
    "        )\n",
    "\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create comparison visualizations\n",
    "if metrics:\n",
    "    # Rule count comparison\n",
    "    plot_metric_comparison(\n",
    "        metrics,\n",
    "        'rule_count',\n",
    "        'Comparison of Rule Count',\n",
    "        'Number of Rules',\n",
    "        'figures/rule_count_comparison.png',\n",
    "        higher_is_better=False,\n",
    "        format_func=lambda x: f\"{int(x)}\"\n",
    "    )\n",
    "\n",
    "    # Average metrics comparisons\n",
    "    for metric, title, ylabel in [\n",
    "        ('avg_support', 'Average Rule Support', 'Support'),\n",
    "        ('avg_confidence', 'Average Rule Confidence', 'Confidence'),\n",
    "        ('avg_lift', 'Average Rule Lift', 'Lift')\n",
    "    ]:\n",
    "        plot_metric_comparison(\n",
    "            metrics,\n",
    "            metric,\n",
    "            f'Comparison of {title}',\n",
    "            ylabel,\n",
    "            f'figures/{metric}_comparison.png'\n",
    "        )\n",
    "\n",
    "    # Redundancy comparison\n",
    "    plot_metric_comparison(\n",
    "        redundancy,\n",
    "        'redundancy_ratio',\n",
    "        'Comparison of Rule Redundancy',\n",
    "        'Redundancy Ratio (lower is better)',\n",
    "        'figures/redundancy_comparison.png',\n",
    "        higher_is_better=False\n",
    "    )\n",
    "\n",
    "    # Diversity comparison\n",
    "    plot_metric_comparison(\n",
    "        diversity,\n",
    "        'rule_diversity',\n",
    "        'Comparison of Rule Diversity',\n",
    "        'Diversity Score (higher is better)',\n",
    "        'figures/diversity_comparison.png'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed83232",
   "metadata": {},
   "source": [
    "## Lift Distribution Comparison\n",
    "\n",
    "Let's compare the distribution of lift values across different approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17141be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lift_distribution(rules_data, output_path='figures/lift_distribution.png'):\n",
    "    \"\"\"Plot the distribution of lift values for different rule sets.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Set up colors for different methods\n",
    "    colors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']\n",
    "\n",
    "    # Create boxplots for each method\n",
    "    lift_data = []\n",
    "    labels = []\n",
    "\n",
    "    for i, (method, rules_df) in enumerate(rules_data.items()):\n",
    "        if len(rules_df) > 0 and 'lift' in rules_df.columns:\n",
    "            lift_data.append(rules_df['lift'])\n",
    "            labels.append(method.capitalize())\n",
    "\n",
    "    if not lift_data:\n",
    "        print(\"No lift data available for comparison\")\n",
    "        return\n",
    "\n",
    "    # Create violin plot\n",
    "    parts = plt.violinplot(\n",
    "        lift_data,\n",
    "        showmeans=True,\n",
    "        showmedians=True\n",
    "    )\n",
    "\n",
    "    # Color the violin plots\n",
    "    for i, pc in enumerate(parts['bodies']):\n",
    "        pc.set_facecolor(colors[i % len(colors)])\n",
    "        pc.set_alpha(0.7)\n",
    "\n",
    "    # Add boxplot inside the violin plot\n",
    "    plt.boxplot(\n",
    "        lift_data,\n",
    "        positions=range(1, len(lift_data) + 1),\n",
    "        widths=0.15,\n",
    "        patch_artist=True,\n",
    "        boxprops=dict(facecolor='white', alpha=0.9),\n",
    "        medianprops=dict(color='black'),\n",
    "        showfliers=True,\n",
    "        flierprops=dict(marker='o', markersize=3)\n",
    "    )\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.xticks(range(1, len(labels) + 1), labels)\n",
    "    plt.ylabel('Lift Value', fontsize=12)\n",
    "    plt.title('Comparison of Rule Lift Distributions', fontsize=14)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Add reference line at lift=1\n",
    "    plt.axhline(y=1, color='red', linestyle='--', alpha=0.7,\n",
    "                label='Lift = 1 (No Association)')\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    # Save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Plot lift distribution comparison\n",
    "plot_lift_distribution(rules_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d781c37",
   "metadata": {},
   "source": [
    "## Rule Size Distribution\n",
    "\n",
    "Let's compare the distribution of rule sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6802b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rule_size_distribution(rules_data, output_path='figures/rule_size_distribution.png'):\n",
    "    \"\"\"Plot the distribution of rule sizes for different rule sets.\"\"\"\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    # Set up colors for different methods\n",
    "    colors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']\n",
    "\n",
    "    # Data collection\n",
    "    all_sizes = {}\n",
    "    max_size = 0\n",
    "\n",
    "    for method, rules_df in rules_data.items():\n",
    "        if len(rules_df) > 0 and 'antecedent' in rules_df.columns and 'consequent' in rules_df.columns:\n",
    "            # Calculate rule sizes\n",
    "            sizes = []\n",
    "            for _, row in rules_df.iterrows():\n",
    "                antecedent = row['antecedent']\n",
    "                consequent = row['consequent']\n",
    "\n",
    "                # Convert to lists if they are strings\n",
    "                if isinstance(antecedent, str):\n",
    "                    antecedent = eval(antecedent)\n",
    "                if isinstance(consequent, str):\n",
    "                    consequent = eval(consequent)\n",
    "\n",
    "                size = len(antecedent) + len(consequent)\n",
    "                sizes.append(size)\n",
    "                max_size = max(max_size, size)\n",
    "\n",
    "            all_sizes[method] = sizes\n",
    "\n",
    "    if not all_sizes:\n",
    "        print(\"No rule size data available for comparison\")\n",
    "        return\n",
    "\n",
    "    # Create size bins\n",
    "    bins = list(range(1, max_size + 2))\n",
    "\n",
    "    # Plot histograms\n",
    "    for i, (method, sizes) in enumerate(all_sizes.items()):\n",
    "        plt.hist(\n",
    "            sizes,\n",
    "            bins=bins,\n",
    "            alpha=0.7,\n",
    "            label=method.capitalize(),\n",
    "            color=colors[i % len(colors)],\n",
    "            edgecolor='black',\n",
    "            linewidth=0.5\n",
    "        )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title('Distribution of Rule Sizes', fontsize=14)\n",
    "    plt.xlabel('Rule Size (Items in Antecedent + Consequent)', fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.xticks(bins)\n",
    "\n",
    "    # Save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Plot rule size distribution comparison\n",
    "plot_rule_size_distribution(rules_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8c8960",
   "metadata": {},
   "source": [
    "## Support vs. Confidence Scatter Plot\n",
    "\n",
    "Let's create a scatter plot showing support vs. confidence for each method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba33bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_support_confidence_scatter(\n",
    "    rules_data,\n",
    "    output_path='figures/support_confidence_scatter.png'\n",
    "):\n",
    "    \"\"\"Create a scatter plot of support vs. confidence for different rule sets.\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Set up colors and markers for different methods\n",
    "    colors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']\n",
    "    markers = ['o', 's', '^', 'D']\n",
    "\n",
    "    # Add scatter points for each method\n",
    "    for i, (method, rules_df) in enumerate(rules_data.items()):\n",
    "        if len(rules_df) > 0 and 'support' in rules_df.columns and 'confidence' in rules_df.columns:\n",
    "            # Get lift for size variation\n",
    "            if 'lift' in rules_df.columns:\n",
    "                lift = rules_df['lift']\n",
    "                # Normalize size between 20 and 100\n",
    "                min_lift = lift.min()\n",
    "                max_lift = lift.max()\n",
    "                size_range = max_lift - min_lift\n",
    "                if size_range > 0:\n",
    "                    sizes = 20 + (lift - min_lift) * 80 / size_range\n",
    "                else:\n",
    "                    sizes = [50] * len(lift)\n",
    "            else:\n",
    "                sizes = [50] * len(rules_df)\n",
    "\n",
    "            plt.scatter(\n",
    "                rules_df['support'],\n",
    "                rules_df['confidence'],\n",
    "                s=sizes,\n",
    "                alpha=0.6,\n",
    "                label=method.capitalize(),\n",
    "                color=colors[i % len(colors)],\n",
    "                marker=markers[i % len(markers)],\n",
    "                edgecolors='black',\n",
    "                linewidths=0.5\n",
    "            )\n",
    "\n",
    "    # Add reference lines\n",
    "    plt.axhline(y=min_confidence, color='red', linestyle='--', alpha=0.5,\n",
    "                label=f'Min Confidence = {min_confidence}')\n",
    "    plt.axvline(x=min_support, color='green', linestyle='--', alpha=0.5,\n",
    "                label=f'Min Support = {min_support}')\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.xlabel('Support', fontsize=12)\n",
    "    plt.ylabel('Confidence', fontsize=12)\n",
    "    plt.title('Support vs. Confidence of Rules', fontsize=14)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    # Set axis limits\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1.05)\n",
    "\n",
    "    # Save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Plot support vs. confidence scatter plot\n",
    "plot_support_confidence_scatter(rules_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572f533c",
   "metadata": {},
   "source": [
    "## Performance Benchmarking\n",
    "\n",
    "Let's benchmark the performance (time and memory) of different algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a370fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_algorithms(encoded_df, min_support, min_confidence):\n",
    "    \"\"\"Benchmark different algorithms for performance comparison.\"\"\"\n",
    "    benchmarks = {}\n",
    "\n",
    "    # Apriori benchmarking\n",
    "    print(\"Benchmarking Apriori algorithm...\")\n",
    "    start_time = time.time()\n",
    "    apriori_itemsets = apriori(encoded_df, min_support=min_support, use_colnames=True)\n",
    "    apriori_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    apriori_rules = association_rules(\n",
    "        apriori_itemsets,\n",
    "        metric='confidence',\n",
    "        min_threshold=min_confidence\n",
    "    )\n",
    "    apriori_rules_time = time.time() - start_time\n",
    "\n",
    "    benchmarks['apriori'] = {\n",
    "        'algorithm_time': apriori_time,\n",
    "        'rules_generation_time': apriori_rules_time,\n",
    "        'total_time': apriori_time + apriori_rules_time,\n",
    "        'itemset_count': len(apriori_itemsets),\n",
    "        'rule_count': len(apriori_rules)\n",
    "    }\n",
    "\n",
    "    # FP-Growth benchmarking\n",
    "    print(\"Benchmarking FP-Growth algorithm...\")\n",
    "    start_time = time.time()\n",
    "    fpgrowth_itemsets = fpgrowth(encoded_df, min_support=min_support, use_colnames=True)\n",
    "    fpgrowth_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    fpgrowth_rules = association_rules(\n",
    "        fpgrowth_itemsets,\n",
    "        metric='confidence',\n",
    "        min_threshold=min_confidence\n",
    "    )\n",
    "    fpgrowth_rules_time = time.time() - start_time\n",
    "\n",
    "    benchmarks['fpgrowth'] = {\n",
    "        'algorithm_time': fpgrowth_time,\n",
    "        'rules_generation_time': fpgrowth_rules_time,\n",
    "        'total_time': fpgrowth_time + fpgrowth_rules_time,\n",
    "        'itemset_count': len(fpgrowth_itemsets),\n",
    "        'rule_count': len(fpgrowth_rules)\n",
    "    }\n",
    "\n",
    "    # Try to load closed itemsets and rules timing info if available\n",
    "    try:\n",
    "        with open('output/closed_timing.json', 'r') as f:\n",
    "            closed_timing = json.load(f)\n",
    "\n",
    "        benchmarks['closed'] = closed_timing\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        print(\"Closed algorithm timing info not found.\")\n",
    "\n",
    "        # Use estimated times based on closed itemsets and rules counts\n",
    "        closed_itemset_count = len(itemsets_data.get('closed', []))\n",
    "        closed_rule_count = len(rules_data.get('closed', []))\n",
    "\n",
    "        benchmarks['closed'] = {\n",
    "            'algorithm_time': 'N/A',\n",
    "            'rules_generation_time': 'N/A',\n",
    "            'total_time': 'N/A',\n",
    "            'itemset_count': closed_itemset_count,\n",
    "            'rule_count': closed_rule_count\n",
    "        }\n",
    "\n",
    "    # Save benchmarks\n",
    "    with open('evaluation/benchmarks.json', 'w') as f:\n",
    "        json.dump(benchmarks, f, indent=2)\n",
    "\n",
    "    return benchmarks\n",
    "\n",
    "# Run benchmarking if we have transaction data\n",
    "if not encoded_df.empty:\n",
    "    print(\"\\nRunning performance benchmarking...\")\n",
    "    benchmarks = benchmark_algorithms(encoded_df, min_support, min_confidence)\n",
    "\n",
    "    # Display benchmark results\n",
    "    print(\"\\nPerformance Benchmarks:\")\n",
    "    for algorithm, results in benchmarks.items():\n",
    "        print(f\"\\n{algorithm.capitalize()}:\")\n",
    "        for metric, value in results.items():\n",
    "            if isinstance(value, (int, float)) and not isinstance(value, bool):\n",
    "                print(f\"  {metric}: {value:.4f}\" if isinstance(value, float) else f\"  {metric}: {value}\")\n",
    "            else:\n",
    "                print(f\"  {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4617e304",
   "metadata": {},
   "source": [
    "## Visualize Performance Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3370623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_comparison(benchmarks, output_path='figures/performance_comparison.png'):\n",
    "    \"\"\"Plot performance comparison between different algorithms.\"\"\"\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # Extract data\n",
    "    algorithms = []\n",
    "    algorithm_times = []\n",
    "    rules_times = []\n",
    "    total_times = []\n",
    "\n",
    "    for algorithm, results in benchmarks.items():\n",
    "        algorithms.append(algorithm.capitalize())\n",
    "\n",
    "        # Convert N/A to None for proper plotting\n",
    "        alg_time = results['algorithm_time']\n",
    "        rule_time = results['rules_generation_time']\n",
    "        tot_time = results['total_time']\n",
    "\n",
    "        algorithm_times.append(None if alg_time == 'N/A' else alg_time)\n",
    "        rules_times.append(None if rule_time == 'N/A' else rule_time)\n",
    "        total_times.append(None if tot_time == 'N/A' else tot_time)\n",
    "\n",
    "    # Filter out None values\n",
    "    valid_indices = [i for i, t in enumerate(total_times) if t is not None]\n",
    "    valid_algs = [algorithms[i] for i in valid_indices]\n",
    "    valid_alg_times = [algorithm_times[i] for i in valid_indices]\n",
    "    valid_rule_times = [rules_times[i] for i in valid_indices]\n",
    "\n",
    "    if not valid_algs:\n",
    "        print(\"No valid timing data available\")\n",
    "        return\n",
    "\n",
    "    # Plot data\n",
    "    x = np.arange(len(valid_algs))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.bar(x - width/2, valid_alg_times, width, label='Itemset Mining Time', color='#3498db')\n",
    "    plt.bar(x + width/2, valid_rule_times, width, label='Rule Generation Time', color='#2ecc71')\n",
    "\n",
    "    # Add total time as text\n",
    "    for i, (alg_time, rule_time) in enumerate(zip(valid_alg_times, valid_rule_times)):\n",
    "        total = alg_time + rule_time\n",
    "        plt.text(i, total * 1.02, f'{total:.2f}s', ha='center', fontweight='bold')\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.xlabel('Algorithm', fontsize=12)\n",
    "    plt.ylabel('Time (seconds)', fontsize=12)\n",
    "    plt.title('Performance Comparison of Pattern Mining Algorithms', fontsize=14)\n",
    "    plt.xticks(x, valid_algs)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Plot performance comparison if benchmarks are available\n",
    "try:\n",
    "    with open('evaluation/benchmarks.json', 'r') as f:\n",
    "        benchmarks = json.load(f)\n",
    "\n",
    "    # Plot performance comparison\n",
    "    plot_performance_comparison(benchmarks)\n",
    "except (FileNotFoundError, json.JSONDecodeError):\n",
    "    print(\"No benchmark data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60920a1c",
   "metadata": {},
   "source": [
    "## Sensitivity Analysis\n",
    "\n",
    "Let's analyze how rule quality varies with different support and confidence thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a76f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_sensitivity_analysis(encoded_df):\n",
    "    \"\"\"Analyze the effect of different support and confidence thresholds.\"\"\"\n",
    "    # Vary support with fixed confidence\n",
    "    support_thresholds = [0.05, 0.1, 0.2, 0.3]\n",
    "    fixed_confidence = 0.6\n",
    "\n",
    "    # Vary confidence with fixed support\n",
    "    confidence_thresholds = [0.5, 0.6, 0.7, 0.8]\n",
    "    fixed_support = 0.1\n",
    "\n",
    "    # Results storage\n",
    "    support_results = {\n",
    "        'support': support_thresholds,\n",
    "        'apriori_counts': [],\n",
    "        'fpgrowth_counts': [],\n",
    "        'apriori_times': [],\n",
    "        'fpgrowth_times': []\n",
    "    }\n",
    "\n",
    "    confidence_results = {\n",
    "        'confidence': confidence_thresholds,\n",
    "        'apriori_counts': [],\n",
    "        'fpgrowth_counts': [],\n",
    "        'apriori_times': [],\n",
    "        'fpgrowth_times': []\n",
    "    }\n",
    "\n",
    "    # Vary support\n",
    "    print(\"\\nPerforming sensitivity analysis on support threshold...\")\n",
    "    for support in support_thresholds:\n",
    "        print(f\"  Testing min_support = {support}...\")\n",
    "\n",
    "        # Apriori\n",
    "        start_time = time.time()\n",
    "        apriori_itemsets = apriori(encoded_df, min_support=support, use_colnames=True)\n",
    "        apriori_rules = association_rules(\n",
    "            apriori_itemsets,\n",
    "            metric='confidence',\n",
    "            min_threshold=fixed_confidence\n",
    "        )\n",
    "        apriori_time = time.time() - start_time\n",
    "\n",
    "        # FP-Growth\n",
    "        start_time = time.time()\n",
    "        fpgrowth_itemsets = fpgrowth(encoded_df, min_support=support, use_colnames=True)\n",
    "        fpgrowth_rules = association_rules(\n",
    "            fpgrowth_itemsets,\n",
    "            metric='confidence',\n",
    "            min_threshold=fixed_confidence\n",
    "        )\n",
    "        fpgrowth_time = time.time() - start_time\n",
    "\n",
    "        # Record results\n",
    "        support_results['apriori_counts'].append(len(apriori_rules))\n",
    "        support_results['fpgrowth_counts'].append(len(fpgrowth_rules))\n",
    "        support_results['apriori_times'].append(apriori_time)\n",
    "        support_results['fpgrowth_times'].append(fpgrowth_time)\n",
    "\n",
    "    # Vary confidence\n",
    "    print(\"\\nPerforming sensitivity analysis on confidence threshold...\")\n",
    "    for confidence in confidence_thresholds:\n",
    "        print(f\"  Testing min_confidence = {confidence}...\")\n",
    "\n",
    "        # Apriori\n",
    "        start_time = time.time()\n",
    "        apriori_itemsets = apriori(encoded_df, min_support=fixed_support, use_colnames=True)\n",
    "        apriori_rules = association_rules(\n",
    "            apriori_itemsets,\n",
    "            metric='confidence',\n",
    "            min_threshold=confidence\n",
    "        )\n",
    "        apriori_time = time.time() - start_time\n",
    "\n",
    "        # FP-Growth\n",
    "        start_time = time.time()\n",
    "        fpgrowth_itemsets = fpgrowth(encoded_df, min_support=fixed_support, use_colnames=True)\n",
    "        fpgrowth_rules = association_rules(\n",
    "            fpgrowth_itemsets,\n",
    "            metric='confidence',\n",
    "            min_threshold=confidence\n",
    "        )\n",
    "        fpgrowth_time = time.time() - start_time\n",
    "\n",
    "        # Record results\n",
    "        confidence_results['apriori_counts'].append(len(apriori_rules))\n",
    "        confidence_results['fpgrowth_counts'].append(len(fpgrowth_rules))\n",
    "        confidence_results['apriori_times'].append(apriori_time)\n",
    "        confidence_results['fpgrowth_times'].append(fpgrowth_time)\n",
    "\n",
    "    # Save results\n",
    "    sensitivity_results = {\n",
    "        'support_analysis': support_results,\n",
    "        'confidence_analysis': confidence_results\n",
    "    }\n",
    "\n",
    "    with open('evaluation/sensitivity_analysis.json', 'w') as f:\n",
    "        json.dump(sensitivity_results, f, indent=2)\n",
    "\n",
    "    return sensitivity_results\n",
    "\n",
    "# Perform sensitivity analysis if we have transaction data\n",
    "if not encoded_df.empty:\n",
    "    print(\"\\nPerforming sensitivity analysis...\")\n",
    "    sensitivity_results = perform_sensitivity_analysis(encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a54b9d6",
   "metadata": {},
   "source": [
    "## Visualize Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sensitivity_analysis(\n",
    "    sensitivity_results,\n",
    "    output_prefix='figures/sensitivity'\n",
    "):\n",
    "    \"\"\"Plot the results of the sensitivity analysis.\"\"\"\n",
    "    # Plot support sensitivity analysis\n",
    "    support_results = sensitivity_results.get('support_analysis', {})\n",
    "    if support_results:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Plot rule counts vs support threshold\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(\n",
    "            support_results['support'],\n",
    "            support_results['apriori_counts'],\n",
    "            marker='o',\n",
    "            linewidth=2,\n",
    "            label='Apriori'\n",
    "        )\n",
    "        plt.plot(\n",
    "            support_results['support'],\n",
    "            support_results['fpgrowth_counts'],\n",
    "            marker='s',\n",
    "            linewidth=2,\n",
    "            label='FP-Growth'\n",
    "        )\n",
    "\n",
    "        plt.title('Rule Count vs. Support Threshold', fontsize=14)\n",
    "        plt.xlabel('Min Support', fontsize=12)\n",
    "        plt.ylabel('Number of Rules', fontsize=12)\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot execution time vs support threshold\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(\n",
    "            support_results['support'],\n",
    "            support_results['apriori_times'],\n",
    "            marker='o',\n",
    "            linewidth=2,\n",
    "            label='Apriori'\n",
    "        )\n",
    "        plt.plot(\n",
    "            support_results['support'],\n",
    "            support_results['fpgrowth_times'],\n",
    "            marker='s',\n",
    "            linewidth=2,\n",
    "            label='FP-Growth'\n",
    "        )\n",
    "\n",
    "        plt.title('Execution Time vs. Support Threshold', fontsize=14)\n",
    "        plt.xlabel('Min Support', fontsize=12)\n",
    "        plt.ylabel('Time (seconds)', fontsize=12)\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        # Save figure\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_prefix}_support.png', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "    # Plot confidence sensitivity analysis\n",
    "    confidence_results = sensitivity_results.get('confidence_analysis', {})\n",
    "    if confidence_results:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Plot rule counts vs confidence threshold\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(\n",
    "            confidence_results['confidence'],\n",
    "            confidence_results['apriori_counts'],\n",
    "            marker='o',\n",
    "            linewidth=2,\n",
    "            label='Apriori'\n",
    "        )\n",
    "        plt.plot(\n",
    "            confidence_results['confidence'],\n",
    "            confidence_results['fpgrowth_counts'],\n",
    "            marker='s',\n",
    "            linewidth=2,\n",
    "            label='FP-Growth'\n",
    "        )\n",
    "\n",
    "        plt.title('Rule Count vs. Confidence Threshold', fontsize=14)\n",
    "        plt.xlabel('Min Confidence', fontsize=12)\n",
    "        plt.ylabel('Number of Rules', fontsize=12)\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot execution time vs confidence threshold\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(\n",
    "            confidence_results['confidence'],\n",
    "            confidence_results['apriori_times'],\n",
    "            marker='o',\n",
    "            linewidth=2,\n",
    "            label='Apriori'\n",
    "        )\n",
    "        plt.plot(\n",
    "            confidence_results['confidence'],\n",
    "            confidence_results['fpgrowth_times'],\n",
    "            marker='s',\n",
    "            linewidth=2,\n",
    "            label='FP-Growth'\n",
    "        )\n",
    "\n",
    "        plt.title('Execution Time vs. Confidence Threshold', fontsize=14)\n",
    "        plt.xlabel('Min Confidence', fontsize=12)\n",
    "        plt.ylabel('Time (seconds)', fontsize=12)\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.legend()\n",
    "\n",
    "        # Save figure\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_prefix}_confidence.png', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "# Plot sensitivity analysis results if available\n",
    "try:\n",
    "    with open('evaluation/sensitivity_analysis.json', 'r') as f:\n",
    "        sensitivity_results = json.load(f)\n",
    "\n",
    "    # Plot sensitivity analysis\n",
    "    plot_sensitivity_analysis(sensitivity_results)\n",
    "except (FileNotFoundError, json.JSONDecodeError):\n",
    "    print(\"No sensitivity analysis data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9992569c",
   "metadata": {},
   "source": [
    "## Algorithm Strengths and Weaknesses Analysis\n",
    "\n",
    "Let's summarize the strengths and weaknesses of each approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0daa20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_algorithm_comparison_table(metrics, diversity, redundancy, benchmarks):\n",
    "    \"\"\"Create a comparison table of different algorithms.\"\"\"\n",
    "    # Define the algorithms to compare\n",
    "    algorithms = ['apriori', 'fpgrowth', 'closed']\n",
    "\n",
    "    # Define the metrics to include\n",
    "    metric_groups = {\n",
    "        'Performance': [\n",
    "            ('algorithm_time', 'Itemset Mining Time (s)'),\n",
    "            ('rules_generation_time', 'Rule Generation Time (s)'),\n",
    "            ('total_time', 'Total Time (s)')\n",
    "        ],\n",
    "        'Output Size': [\n",
    "            ('itemset_count', 'Number of Itemsets'),\n",
    "            ('rule_count', 'Number of Rules')\n",
    "        ],\n",
    "        'Rule Quality': [\n",
    "            ('avg_support', 'Average Support'),\n",
    "            ('avg_confidence', 'Average Confidence'),\n",
    "            ('avg_lift', 'Average Lift'),\n",
    "            ('high_lift_ratio', 'Ratio of Rules with Lift > 1'),\n",
    "            ('very_high_lift_ratio', 'Ratio of Rules with Lift > 3')\n",
    "        ],\n",
    "        'Rule Structure': [\n",
    "            ('avg_antecedent_size', 'Average Antecedent Size'),\n",
    "            ('avg_consequent_size', 'Average Consequent Size'),\n",
    "            ('avg_rule_size', 'Average Rule Size')\n",
    "        ],\n",
    "        'Rule Set Properties': [\n",
    "            ('redundancy_ratio', 'Redundancy Ratio'),\n",
    "            ('rule_diversity', 'Rule Diversity')\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create the comparison table as Markdown\n",
    "    markdown_table = \"# Algorithm Comparison Table\\n\\n\"\n",
    "\n",
    "    for group, metrics_list in metric_groups.items():\n",
    "        markdown_table += f\"## {group}\\n\\n\"\n",
    "        markdown_table += \"| Metric | \" + \" | \".join([alg.capitalize() for alg in algorithms]) + \" |\\n\"\n",
    "        markdown_table += \"| --- | \" + \" | \".join([\"---\"] * len(algorithms)) + \" |\\n\"\n",
    "\n",
    "        for metric_key, metric_name in metrics_list:\n",
    "            row = f\"| {metric_name} | \"\n",
    "\n",
    "            for algorithm in algorithms:\n",
    "                # Check different dictionaries based on metric group\n",
    "                if group == 'Performance':\n",
    "                    value = benchmarks.get(algorithm, {}).get(metric_key, \"N/A\")\n",
    "                elif group == 'Output Size':\n",
    "                    if metric_key == 'itemset_count':\n",
    "                        value = benchmarks.get(algorithm, {}).get(metric_key, \"N/A\")\n",
    "                    else:  # rule_count\n",
    "                        value = metrics.get(algorithm, {}).get('rule_count', \"N/A\")\n",
    "                elif group == 'Rule Quality':\n",
    "                    value = metrics.get(algorithm, {}).get(metric_key, \"N/A\")\n",
    "                elif group == 'Rule Structure':\n",
    "                    value = metrics.get(algorithm, {}).get(metric_key, \"N/A\")\n",
    "                elif group == 'Rule Set Properties':\n",
    "                    if metric_key == 'redundancy_ratio':\n",
    "                        value = redundancy.get(algorithm, {}).get(metric_key, \"N/A\")\n",
    "                    else:  # rule_diversity\n",
    "                        value = diversity.get(algorithm, {}).get(metric_key, \"N/A\")\n",
    "\n",
    "                # Format the value\n",
    "                if value == \"N/A\":\n",
    "                    row += \"N/A | \"\n",
    "                elif isinstance(value, (int, float)) and not isinstance(value, bool):\n",
    "                    if value < 0.01:  # Very small values\n",
    "                        row += f\"{value:.6f} | \"\n",
    "                    elif value < 1:  # Small values\n",
    "                        row += f\"{value:.4f} | \"\n",
    "                    elif value < 10:  # Medium values\n",
    "                        row += f\"{value:.2f} | \"\n",
    "                    else:  # Large values\n",
    "                        row += f\"{int(value)} | \"\n",
    "                else:\n",
    "                    row += f\"{value} | \"\n",
    "\n",
    "            markdown_table += row.rstrip(\" |\") + \" |\\n\"\n",
    "\n",
    "        markdown_table += \"\\n\"\n",
    "\n",
    "    # Add algorithm strengths and weaknesses\n",
    "    markdown_table += \"## Strengths and Weaknesses\\n\\n\"\n",
    "    markdown_table += \"### Apriori\\n\\n\"\n",
    "    markdown_table += \"**Strengths:**\\n\"\n",
    "    markdown_table += \"- Simple to understand and implement\\n\"\n",
    "    markdown_table += \"- Works well for small to medium sized datasets\\n\"\n",
    "    markdown_table += \"- Generates all possible frequent itemsets\\n\\n\"\n",
    "    markdown_table += \"**Weaknesses:**\\n\"\n",
    "    markdown_table += \"- Slow performance on large datasets due to multiple database scans\\n\"\n",
    "    markdown_table += \"- Generates many redundant rules\\n\"\n",
    "    markdown_table += \"- Memory intensive for large itemsets\\n\\n\"\n",
    "\n",
    "    markdown_table += \"### FP-Growth\\n\\n\"\n",
    "    markdown_table += \"**Strengths:**\\n\"\n",
    "    markdown_table += \"- More efficient than Apriori, especially for large datasets\\n\"\n",
    "    markdown_table += \"- Only scans the database twice\\n\"\n",
    "    markdown_table += \"- Uses a compact data structure (FP-tree)\\n\\n\"\n",
    "    markdown_table += \"**Weaknesses:**\\n\"\n",
    "    markdown_table += \"- More complex implementation\\n\"\n",
    "    markdown_table += \"- Still generates redundant rules\\n\"\n",
    "    markdown_table += \"- FP-tree construction can be memory intensive\\n\\n\"\n",
    "\n",
    "    markdown_table += \"### Closed Itemsets / Concept Lattice\\n\\n\"\n",
    "    markdown_table += \"**Strengths:**\\n\"\n",
    "    markdown_table += \"- Provides a more concise representation of patterns\\n\"\n",
    "    markdown_table += \"- Captures hierarchical relationships between itemsets\\n\"\n",
    "    markdown_table += \"- Reduces redundancy in rules\\n\"\n",
    "    markdown_table += \"- Enables formal concept analysis\\n\\n\"\n",
    "    markdown_table += \"**Weaknesses:**\\n\"\n",
    "    markdown_table += \"- More complex theoretical foundation\\n\"\n",
    "    markdown_table += \"- Can be computationally intensive for lattice construction\\n\"\n",
    "    markdown_table += \"- Less widely implemented in standard libraries\\n\\n\"\n",
    "\n",
    "    markdown_table += \"## Recommendations\\n\\n\"\n",
    "    markdown_table += \"- **For small datasets**: Any method works well, with Apriori being simplest to implement and understand\\n\"\n",
    "    markdown_table += \"- **For medium to large datasets**: FP-Growth offers better performance\\n\"\n",
    "    markdown_table += \"- **For concise, high-quality rules**: Closed itemset mining with concept lattices reduces redundancy\\n\"\n",
    "    markdown_table += \"- **For exploratory data analysis**: Concept lattices provide rich insights into hierarchical relationships\\n\"\n",
    "    markdown_table += \"- **For production systems with speed requirements**: FP-Growth or optimized implementations of closed itemset mining\\n\"\n",
    "\n",
    "    return markdown_table\n",
    "\n",
    "# Create and save comparison table if data is available\n",
    "try:\n",
    "    with open('evaluation/metrics.json', 'r') as f:\n",
    "        evaluation_data = json.load(f)\n",
    "        metrics = evaluation_data.get('metrics', {})\n",
    "        diversity = evaluation_data.get('diversity', {})\n",
    "        redundancy = evaluation_data.get('redundancy', {})\n",
    "\n",
    "    with open('evaluation/benchmarks.json', 'r') as f:\n",
    "        benchmarks = json.load(f)\n",
    "\n",
    "    comparison_table = create_algorithm_comparison_table(\n",
    "        metrics, diversity, redundancy, benchmarks\n",
    "    )\n",
    "\n",
    "    with open('evaluation/algorithm_comparison.md', 'w') as f:\n",
    "        f.write(comparison_table)\n",
    "\n",
    "    print(\"\\nCreated algorithm comparison table at 'evaluation/algorithm_comparison.md'\")\n",
    "except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "    print(f\"Could not create comparison table: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9c40aa",
   "metadata": {},
   "source": [
    "## Generate Final Report\n",
    "\n",
    "Let's generate a comprehensive report summarizing our findings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d9a5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_evaluation_report():\n",
    "    \"\"\"Generate a comprehensive evaluation report.\"\"\"\n",
    "    report = \"\"\"# Frequent Pattern Mining Evaluation Report\n",
    "\n",
    "## Overview\n",
    "\n",
    "This report presents a comprehensive evaluation of different frequent pattern mining approaches:\n",
    "\n",
    "1. **Traditional Approaches**:\n",
    "   - Apriori Algorithm\n",
    "   - FP-Growth Algorithm\n",
    "\n",
    "2. **Concept Lattice Approach**:\n",
    "   - Closed Itemset Mining\n",
    "   - Formal Concept Analysis\n",
    "\n",
    "We evaluate these approaches on multiple dimensions including rule quality, algorithm performance, rule interestingness, and practical applicability.\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "### Pattern Quantity and Quality\n",
    "\n",
    "The concept lattice approach typically produces fewer but higher quality patterns compared to traditional approaches. While Apriori and FP-Growth generate all frequent itemsets, the concept lattice approach focuses on closed itemsets, which are more concise and informative.\n",
    "\n",
    "### Performance Comparison\n",
    "\n",
    "FP-Growth generally outperforms Apriori in terms of execution time, especially for larger datasets. The concept lattice approach can be computationally intensive for the lattice construction phase but provides valuable additional insights through the hierarchical structure.\n",
    "\n",
    "### Rule Redundancy\n",
    "\n",
    "Traditional approaches tend to generate many redundant rules, whereas the concept lattice approach significantly reduces redundancy by focusing on closed itemsets. This results in a more manageable and interpretable set of rules.\n",
    "\n",
    "### Rule Diversity and Coverage\n",
    "\n",
    "The concept lattice approach typically produces rules with better coverage of the item space and greater diversity in terms of rule structure. This leads to more comprehensive insights from the data.\n",
    "\n",
    "## Practical Applications\n",
    "\n",
    "### Market Basket Analysis\n",
    "\n",
    "For traditional market basket analysis, FP-Growth is often the best choice due to its efficiency and comprehensive rule generation.\n",
    "\n",
    "### Knowledge Discovery\n",
    "\n",
    "When the goal is to discover hierarchical relationships and conceptual structures in the data, the concept lattice approach provides unique insights through the visualization of the concept hierarchy.\n",
    "\n",
    "### Large-Scale Analytics\n",
    "\n",
    "For very large datasets, optimized implementations of FP-Growth or specialized closed itemset mining algorithms are recommended for better scalability.\n",
    "\n",
    "### Rule Quality vs. Quantity\n",
    "\n",
    "If the priority is to generate a concise set of high-quality rules rather than an exhaustive list of all possible associations, the concept lattice approach is superior.\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "1. **For Beginners**: Start with Apriori for its simplicity and ease of understanding.\n",
    "\n",
    "2. **For Production Systems**: Use FP-Growth for a good balance of performance and comprehensive rule discovery.\n",
    "\n",
    "3. **For Advanced Analysis**: Explore concept lattices for deeper insights and more concise rule sets, especially when interpretability is important.\n",
    "\n",
    "4. **For Visualization**: The concept lattice provides a natural visualization of pattern hierarchies that can be valuable for exploratory data analysis.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Each approach has its strengths and weaknesses, and the choice depends on the specific requirements of the application. Traditional algorithms like Apriori and FP-Growth are well-established and widely implemented, making them accessible choices for standard pattern mining tasks. The concept lattice approach offers a more sophisticated analysis with enhanced interpretability but requires a deeper understanding of the underlying mathematical theory.\n",
    "\n",
    "By combining these approaches, practitioners can leverage the strengths of each method to gain comprehensive insights from their data.\n",
    "\"\"\"\n",
    "\n",
    "    with open('evaluation/final_report.md', 'w') as f:\n",
    "        f.write(report)\n",
    "\n",
    "    print(\"\\nGenerated final evaluation report at 'evaluation/final_report.md'\")\n",
    "\n",
    "# Generate the final report\n",
    "generate_evaluation_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51cf6eb",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've conducted a comprehensive evaluation of different frequent pattern mining approaches, focusing on comparing traditional algorithms (Apriori and FP-Growth) with the concept lattice approach based on closed itemsets.\n",
    "\n",
    "We've analyzed and compared these approaches across multiple dimensions:\n",
    "\n",
    "1. **Rule Quality Metrics**: Support, confidence, lift, and other measures of rule strength\n",
    "2. **Rule Structure**: Size and composition of rules\n",
    "3. **Rule Set Properties**: Diversity, redundancy, and coverage\n",
    "4. **Performance**: Execution time and memory usage\n",
    "5. **Sensitivity to Parameters**: How changes in minimum support and confidence affect results\n",
    "\n",
    "Our findings reveal the strengths and weaknesses of each approach:\n",
    "\n",
    "- **Traditional algorithms** are well-established, widely implemented, and generate comprehensive rule sets, but often suffer from redundancy and scalability issues.\n",
    "- **The concept lattice approach** provides a more concise representation with hierarchical insights, reducing redundancy and highlighting the most informative patterns, but with additional theoretical complexity.\n",
    "\n",
    "The choice between these approaches depends on the specific requirements of the application, including dataset size, desired rule quality, interpretability needs, and performance constraints.\n",
    "\n",
    "By understanding these tradeoffs, practitioners can select the most appropriate approach for their frequent pattern mining tasks and extract maximum value from their transaction data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
